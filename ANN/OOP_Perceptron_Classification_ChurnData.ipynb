{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d4f2eb7-e08b-456a-b797-dddf11c64bff",
   "metadata": {},
   "source": [
    "#### Exercise 1 - Using OO design, Implement a classification using Perceptron\n",
    "##### Task-1: Load and Explore Churn Modeling Dataset\n",
    "##### Task-2: Prepare Feature Matrix and Target Variable for Churn Modeling Prediction\n",
    "##### Task-3: Standardize Input Features Using Standard Scaler\n",
    "##### Task-4: Split the Dataset into Training and Testing Sets\n",
    "##### Task-5: Initialize Model Parameters for Classification\n",
    "##### Task-6: Implement Activation Function (step function) for Perceptron Model\n",
    "##### Task-7: Implement Forward Propagation with Activation Function for Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a34db55-c762-43ef-8f7f-91339ede7461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b7e10-191b-48b0-9587-ca199b813655",
   "metadata": {},
   "source": [
    "## Task-1: Load and Explore Churn Modeling Dataset\n",
    "\n",
    "Load the dataset from the CSV file located at '/shareddata/datasets/ngitcourse4/Churn_Modelling_Data.csv' into a pandas DataFrame called 'churn_df'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c126f447-e1b8-4306-b695-d23696a6a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df = pd.read_csv('Churn_Modelling_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db225ab4-1f6b-4455-b61d-b4b4cd9eb52a",
   "metadata": {},
   "source": [
    "## Task-2: Prepare Feature Matrix and Target Variable for Churn Modeling Prediction\n",
    "\n",
    "Steps:\n",
    "\n",
    "1) Drop the Exited column from the DataFrame churn_df to isolate the input features.\n",
    "2) Extract the Exited column from churn_df as the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2264938c-5fd7-4649-a314-a990d9249c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = churn_df.drop(['Exited'], axis=1).values\n",
    "y = churn_df['Exited'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a071b1eb-ec2f-4085-a786-a75b18340060",
   "metadata": {},
   "source": [
    "## Task-3: Standardize Input Features Using Standard Scaler\n",
    "\n",
    "Standardize the input features (X) to have a mean of 0 and a standard deviation of 1, \n",
    "using the StandardScaler from sklearn.preprocessing.\n",
    "\n",
    "Steps:\n",
    "    \n",
    "1) Initialize the StandardScaler from the sklearn.preprocessing module\n",
    "2) Apply the scaler to the input feature matrix X by fitting the scaler to the data and then transforming it to standardized values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1718519a-d457-4719-9be7-f0dceb3450a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2cc26c-a172-40e2-aafe-afdc6c15aa36",
   "metadata": {},
   "source": [
    "## Task-4: Split the Dataset into Training and Testing Sets\n",
    "\n",
    "Split the standardized input features (X_scaled) and target variable (y) into training and testing sets \n",
    "using the train_test_split function from sklearn.model_selection. \n",
    "\n",
    "Steps:\n",
    "\n",
    "1) Use the train_test_split function to divide the standardized input data (X_scaled) and \n",
    "the target variable (y) into training and testing sets.\n",
    "2) Specify test_size=0.2 to allocate 20% of the data for testing and 80% for training.\n",
    "3) Set random_state=42 to ensure that the split is reproducible, so that running the code multiple times yields the same split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80118d9c-f10c-4d07-8613-cb33c513c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0086093-a144-4b53-b71d-818d2d7d849b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe82a1c6-c307-4664-a7df-bf859fbd2969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15a377de-8c6d-4ee1-ae98-1dd2ce88fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def initialize_parameters(self, X):\n",
    "        pass\n",
    "    \n",
    "    def activation(self, z):\n",
    "        pass\n",
    "    \n",
    "    def activation_derivative(self, z):\n",
    "        pass \n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        pass\n",
    "    \n",
    "    def compute_bce(self,y, y_pred):\n",
    "        pass\n",
    "    \n",
    "    def bce_derivative(self,y, y_pred):\n",
    "        pass\n",
    "    \n",
    "    def compute_gradients(self, X, y_true, y_pred):\n",
    "        pass\n",
    "    \n",
    "    def train(self, X, num_epochs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb3a9a-94c7-4ef3-9ef8-05f7cf462de0",
   "metadata": {},
   "source": [
    "## Task-5: Initialize Model Parameters for Classification\n",
    "\n",
    "Implement a function to initialize the weights and bias parameters for a classification model. \n",
    "The weights are initialized randomly with small values, and the bias is set to zero.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1) Define a function initialize_parameters that takes the input data X as an argument.\n",
    "Inside the function:\n",
    "2) Calculate the number of features in X \n",
    "3) Initialize the weights as a NumPy array of shape using np.random.randn\n",
    "4) Set the bias to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc748aa6-7214-46d5-ac61-ce9c12a1f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(self, X):\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    # Initializing weights and bias\n",
    "    self.weights = np.random.randn(n_features, 1) * 0.01\n",
    "    self.bias = 0\n",
    "    \n",
    "Perceptron.initialize_parameters=initialize_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12335b9e-f3c5-4732-8976-da2f61846169",
   "metadata": {},
   "source": [
    "## Task-6: Implement Activation Function (step function) for Perceptron Model\n",
    "\n",
    "Objective:\n",
    "\n",
    "Define the step function as the activation function for the Perceptron model, which will be used to determine the binary output (0 or 1) based on the linear combination of inputs and weights.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1) Define an activation function that takes self and z as arguments:\n",
    "z is the input to the activation function, which is the result of the dot product between the input features and the weights plus the bias.\n",
    "2) Inside the function, apply the step function outputs 1 if z >= 0 and 0 if z < 0. \n",
    "This function converts the continuous output of the linear model into a binary class label (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a80465a-de69-4abf-bcbd-de57c91f6794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(self, z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "Perceptron.activation=activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb420cfd-3f4f-45e7-8e2e-c20b62ddee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_derivative(self, z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    a = self.activation(z)\n",
    "    \n",
    "    return a * (1 - a)\n",
    "\n",
    "Perceptron.activation_derivative=activation_derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076154f2-7197-4b12-b88c-8e397f980c88",
   "metadata": {},
   "source": [
    "## Task-7: Implement Forward Propagation with Activation Function for Perceptron\n",
    "\n",
    "Implement forward propagation for the Perceptron model, where the input features are linearly combined \n",
    "with the weights and bias, and then passed through the activation function to generate binary predictions.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1) Define the forward_propagation method, which takes X as an argument.\n",
    "Inside the method:\n",
    "2) Compute the linear combination of the input feature matrix X with the weights (self.weights) and add the bias (self.bias):\n",
    "z = np.dot(X, self.weights) + self.bias\n",
    "This step produces the raw output (z) of the model, representing the input to the activation function.\n",
    "3) Pass the computed z through the activation function (self.activation(z)) to get the predicted labels (y_pred):\n",
    "The activation function (such as the step function) converts the continuous output into binary values (0 or 1) for classification.\n",
    "Return the predicted values y_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4808c54-5654-4629-a0bd-c2fdadec40c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(self, X):\n",
    "   \n",
    "    # compute the sum of the product of the input and the weights\n",
    "    z = np.dot(X, self.weights) + self.bias # Finding the dot product and adding the bias\n",
    "    y_pred = self.activation(z) # Passing through an activation function\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "Perceptron.forward_propagation=forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f878fb92-584d-4de1-9fc5-179fd10524af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bce(self, y, y_pred):\n",
    "    m = y.shape[0]\n",
    "    bce = (1/m) * np.sum(-(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred)))\n",
    "    \n",
    "    return bce\n",
    "\n",
    "Perceptron.compute_bce=compute_bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52c42ca3-950e-425a-8d3b-ba5e7423d233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_derivative(self, y, y_pred):\n",
    "     m = y.shape[0]\n",
    "   \n",
    "     return (1/m) * (y_pred - y) / (y_pred * (1 - y_pred))\n",
    "\n",
    "Perceptron.bce_derivative=bce_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4162a0f8-adf4-4a47-a1e9-790de5134e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the gradient of BCE with respect to the parameters (w, b)\n",
    "def compute_gradients(self, X, y_true, y_pred):\n",
    " \n",
    "    d_pred_loss = self.bce_derivative(y_true, y_pred)             # Derivative of loss w.r.t y_pred\n",
    "    d_activation_loss = self.activation_derivative(y_pred)           # Derivative of sigmoid activation\n",
    "    dL_dz = d_pred_loss * d_activation_loss                  # Chain rule: derivative w.r.t z\n",
    "    dL_dw = np.dot(X.T, dL_dz) / X.shape[0]                  # Gradient w.r.t weights\n",
    "    dL_db = np.mean(dL_dz, axis=0)                           # Gradient w.r.t bias\n",
    "    \n",
    "    return dL_dw, dL_db\n",
    "\n",
    "Perceptron.compute_gradients=compute_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf341b9e-5305-4e51-a167-f8b6a3c50a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, X, num_epochs):\n",
    "    \n",
    "    learning_rate=0.01 # 0.01, 0.1, 0.5\n",
    "    loss = 0\n",
    "    \n",
    "    for epoch in range(0, num_epochs):\n",
    "        \n",
    "        y_pred = self.forward_propagation(X).reshape(-1,1)\n",
    "        loss = self.compute_bce(y_train, y_pred)\n",
    "       \n",
    "        # Compute the gradients\n",
    "        dw, db = self.compute_gradients(X, y_train, y_pred)\n",
    "\n",
    "        # update parameters W (weights) and b (bias)\n",
    "\n",
    "        self.weights = self.weights - dw * learning_rate\n",
    "        self.bias = self.bias - db * learning_rate\n",
    "\n",
    "        print(f\"Epoch {epoch+1}    loss = {loss}\")\n",
    "    return loss   \n",
    "\n",
    "Perceptron.train=train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc2baf06-00ea-4a25-8da8-785c92d9ac4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00426966]\n",
      " [-0.000111  ]\n",
      " [-0.00115684]\n",
      " [ 0.00741583]\n",
      " [-0.01607478]\n",
      " [-0.00317835]\n",
      " [-0.0156156 ]\n",
      " [-0.0201802 ]\n",
      " [-0.00717695]\n",
      " [-0.02023813]]\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.initialize_parameters(X_train)\n",
    "print(perceptron.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d929c30c-cbc7-4dd3-903d-9c3412db2f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6930996918819761"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = perceptron.forward_propagation(X_train).reshape(-1,1)\n",
    "perceptron.compute_bce(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d965ec26-fa44-4a99-a755-79643a038022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_weights_bias(steps):\n",
    "    for i in range(1, steps):\n",
    "        perceptron.initialize_parameters(X_train)\n",
    "        y_pred = perceptron.forward_propagation(X_train).reshape(-1,1)\n",
    "        bce = perceptron.compute_bce(y_train, y_pred)\n",
    "        print(f\"bce loss = {bce}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ac947bf-72b8-40af-aee6-7001e5b5a816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bce loss = 0.6922137060594854\n",
      "bce loss = 0.6960018224322305\n",
      "bce loss = 0.6941105934058425\n",
      "bce loss = 0.6917042192788531\n",
      "bce loss = 0.692950631544614\n",
      "bce loss = 0.6963590418316041\n",
      "bce loss = 0.6950704942348961\n",
      "bce loss = 0.6947810679617461\n",
      "bce loss = 0.6918802915155332\n"
     ]
    }
   ],
   "source": [
    "random_weights_bias(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4651dc78-a0a1-400a-9c07-ba3d4de139a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1    loss = 0.6918802915155332\n",
      "Epoch 2    loss = 0.6918717308381204\n",
      "Epoch 3    loss = 0.6918631705027233\n",
      "Epoch 4    loss = 0.6918546105093237\n",
      "Epoch 5    loss = 0.6918460508579035\n",
      "Epoch 6    loss = 0.6918374915484446\n",
      "Epoch 7    loss = 0.6918289325809286\n",
      "Epoch 8    loss = 0.6918203739553375\n",
      "Epoch 9    loss = 0.6918118156716528\n",
      "Epoch 10    loss = 0.6918032577298568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6918032577298568"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron.train(X_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36295b4e-f288-425f-a077-d424aefc7582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
