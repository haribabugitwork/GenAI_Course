{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c62e2b9-b4c8-40a7-9e8b-6beb362324a1",
   "metadata": {},
   "source": [
    "#### Exercise 1 - Gradient Descent: Training and optimization\n",
    "##### Task-1: Compute the gradient of quadratic loss function\n",
    "##### Task-2: Train and update parameter to minimize the loss\n",
    "##### Task-3: Compute the partial derivatives of quadratic loss function\n",
    "##### Task-4: Train and update two parameters to minimize the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7967def0-6af4-4278-ae00-f761e9bf0375",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed9f766a-cbc1-4113-be2c-e30722402d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ff5e3cb-10e7-4c98-a502-f57aecaacfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(x):\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f91552-25d1-4d76-9953-9369ff3fd488",
   "metadata": {},
   "source": [
    "### Task-1: Compute the derivative of loss_fun(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9721ef6-0cae-4025-8ddb-43d711726dba",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-54324e617e6f4294",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn_derivative(x):\n",
    "  \n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b23ac7dc-7579-47fe-a9cc-5defaf63c4d2",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d9326a95ba3c34ba",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = 3\n",
    "expected_output = 6\n",
    "assert loss_fn_derivative(x) == expected_output, f\"Expected {expected_output}, got {loss_fn_derivative(x)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "affda5c8-5ad9-45c7-a242-2e6500b381b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001   # try with 0.01, 0.001, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37ed9d41-62fb-4db9-99ad-59a6cf42facf",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d5807c58cd537986",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caea01e3-c872-47dc-9d7a-7eb185a67a2e",
   "metadata": {},
   "source": [
    "### Task-2: Update parameter using the gradient and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18cc1cdb-f17a-4fdf-abc8-2fa6d88162e3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f65c025420f1d30e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x = 7.98   Loss = 63.74\n",
      " x = 7.97   Loss = 63.49\n",
      " x = 7.95   Loss = 63.24\n",
      " x = 7.94   Loss = 62.98\n",
      " x = 7.92   Loss = 62.73\n",
      " x = 7.90   Loss = 62.48\n",
      " x = 7.89   Loss = 62.23\n",
      " x = 7.87   Loss = 61.98\n",
      " x = 7.86   Loss = 61.73\n",
      " x = 7.84   Loss = 61.49\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    # Update the variable x by moving it in the direction that minimizes the loss. \n",
    "    # Use the learning rate and the derivative of the loss function to adjust x. \n",
    "    \n",
    "    x = x - learning_rate * loss_fn_derivative(x)\n",
    "    \n",
    "    loss = loss_fn(x)\n",
    "    print(f\" x = {x:0.2f}   Loss = {loss:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbc0ff69-a0c3-4734-89a1-d7e483b8befd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn2(x1, x2):\n",
    "    return (x1-x2) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e830794-41cd-4b72-9c3c-a6e736739c79",
   "metadata": {},
   "source": [
    "### Task-3: Return the partial derivatives of loss_fn2(x1, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2186764-b425-42f5-be9e-28877bc327a7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e1656332eded4f82",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn2_derivative(x1, x2):\n",
    "    grad_x1 = 2*(x1-x2)\n",
    "    # Compute partial derivative w.r.t x2\n",
    "    grad_x2 = 2*(x1-x2) * -1\n",
    "\n",
    "    return np.array([grad_x1, grad_x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7a56c51-72fc-4248-93e4-3ee68537e4b6",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2d3f83b012d9a04d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test case\n",
    "x1_t, x2_t = 5, 3\n",
    "expected_grad_x1 = 4\n",
    "expected_grad_x2 = -4\n",
    "grad = loss_fn2_derivative(x1_t, x2_t)\n",
    "assert grad[0] == expected_grad_x1, f\"Expected grad_x1 to be {expected_grad_x1}, but got {grad[0]}\"\n",
    "assert grad[1] == expected_grad_x2, f\"Expected grad_x2 to be {expected_grad_x2}, but got {grad[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5ac4a4d-807c-4275-b509-02f0d00b86b4",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-147fc2ced36fcc9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x1 = 8\n",
    "x2 = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd119e9d-afaa-4529-8182-678ef76e7218",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-29d1b25d28160bf4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, -8])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn2_derivative(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c5f891-104b-4b1c-8a9f-61fcf1944744",
   "metadata": {},
   "source": [
    "### Task-4: Update parameters using the partial derivative(s) and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7fabd03-7ba8-4030-b16a-4470187d95d3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-986d3cbf838711f2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x1 = 7.99, x2 = 4.01     Loss = 15.87\n",
      " x1 = 7.98, x2 = 4.02     Loss = 15.75\n",
      " x1 = 7.98, x2 = 4.02     Loss = 15.62\n",
      " x1 = 7.97, x2 = 4.03     Loss = 15.50\n",
      " x1 = 7.96, x2 = 4.04     Loss = 15.37\n",
      " x1 = 7.95, x2 = 4.05     Loss = 15.25\n",
      " x1 = 7.94, x2 = 4.06     Loss = 15.13\n",
      " x1 = 7.94, x2 = 4.06     Loss = 15.01\n",
      " x1 = 7.93, x2 = 4.07     Loss = 14.89\n",
      " x1 = 7.92, x2 = 4.08     Loss = 14.77\n",
      " x1 = 7.91, x2 = 4.09     Loss = 14.65\n",
      " x1 = 7.91, x2 = 4.09     Loss = 14.53\n",
      " x1 = 7.90, x2 = 4.10     Loss = 14.42\n",
      " x1 = 7.89, x2 = 4.11     Loss = 14.30\n",
      " x1 = 7.88, x2 = 4.12     Loss = 14.19\n",
      " x1 = 7.88, x2 = 4.12     Loss = 14.08\n",
      " x1 = 7.87, x2 = 4.13     Loss = 13.96\n",
      " x1 = 7.86, x2 = 4.14     Loss = 13.85\n",
      " x1 = 7.85, x2 = 4.15     Loss = 13.74\n",
      " x1 = 7.85, x2 = 4.15     Loss = 13.63\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    x1 = x1 - learning_rate * loss_fn2_derivative(x1, x2)[0]\n",
    "    # Update parameter x2, use partial derivative w.r.t x2 and learning_rate\n",
    "    x2 = x2 - learning_rate * loss_fn2_derivative(x1, x2)[1]\n",
    "   \n",
    "    loss1 = loss_fn2(x1, x2)\n",
    "    print(f\" x1 = {x1:0.2f}, x2 = {x2:0.2f}     Loss = {loss1:0.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
