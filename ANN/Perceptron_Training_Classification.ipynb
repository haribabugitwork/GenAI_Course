{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a731f3b-702f-45e2-98f9-d3cd85e43304",
   "metadata": {},
   "source": [
    "# Binary Classification with Perceptron: BackPropagation, Gradient Descent and Training\n",
    "\n",
    "This notebook demonstrates the basics of deep learning, including forward propagation, loss calculation, and backpropagation. Lets build a simple neural network from scratch using NumPy to grasp the underlying concepts without relying on high-level libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b27d2-c55c-4954-bccf-a5bd905e03d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6dd0bb-0261-41e3-bac7-a032cef1c47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a80bef7-f643-4832-b1ff-399fed5c0a1f",
   "metadata": {},
   "source": [
    "#### Exercise 1 - Classification: Training and optimization for Perceptron\n",
    "##### Task-1: Compute the gradient of BCE with respect to the parameters (w, b)\n",
    "##### Task-2: Update parameters W (weights) and b (bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692c50b1-e873-4dfc-b22f-5a4203b6d3c4",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "We'll use NumPy for numerical computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3cee6d6-e88d-498c-b9cd-442b19706f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b110b-a7e9-4e66-92ee-27f638bdc5c3",
   "metadata": {},
   "source": [
    "## Step 2: Define Activation Functions\n",
    "\n",
    "We'll use the sigmoid activation function and its derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac3cd33d-ead2-4bdd-bc95-cfa049c5a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    \n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a2215f-8c85-46f1-9784-69b56a60d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    a = sigmoid(x)\n",
    "    \n",
    "    return a * (1 - a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae0e67-1ccb-497a-a9a3-8990d88f186c",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Parameters\n",
    "\n",
    "We'll set up input features, target outputs, and initialize weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00701377-d59a-4821-bbfa-07f278b12c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data (features) and true label\n",
    "X = np.array([\n",
    "    [0.5, 1.5],\n",
    "    [1.0, 2.0],\n",
    "    [1.5, 0.5],\n",
    "    [3.0, 1.0]\n",
    "])  # Input: 4 samples, 2 features each\n",
    "y_true = np.array([[0], [0], [1], [1]])  # True output labels\n",
    "\n",
    "# Initialize weights and bias randomly\n",
    "np.random.seed(0)  # For reproducibility\n",
    "W = np.random.randn(2, 1)  # Weights: 2 inputs to 1 output neuron\n",
    "b = np.random.randn(1)     # Bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c544d2-a88e-4f08-93a1-b93c7cb8904a",
   "metadata": {},
   "source": [
    "## Step 4: Forward Propagation\n",
    "\n",
    "Compute the outputs of the perceptron layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fca0ca42-9a07-4f48-a253-ce5b7875cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, W, b):\n",
    "   \n",
    "    # compute the sum of the product of the input and the weights\n",
    "    z = np.dot(X, W) + b\n",
    "    y_pred = sigmoid(z)       # Sigmoid activation function\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243a37b-19b6-4db4-84f0-3bda08cd4de0",
   "metadata": {},
   "source": [
    "## Step 5: Compute Loss\n",
    "\n",
    "We'll use Binary Cross Entropy (BCE) to measure the difference between predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82f46d19-0e3c-4f4b-b881-6b07327c82ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bce(y, y_pred):\n",
    "    m = y.shape[0]\n",
    "    bce = (1/m) * np.sum(-(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)))\n",
    "    \n",
    "    return bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "896a7861-2146-4d34-b735-28ce8c37e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_derivative(y, y_pred):\n",
    "     m = y.shape[0]\n",
    "   \n",
    "     return (1/m) * (y_pred - y) / (y_pred * (1 - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15274eec-8fca-470f-82fd-91b0333079a4",
   "metadata": {},
   "source": [
    "## Step 6: Backward Propagation\n",
    "\n",
    "Calculate gradients and update weights and biases accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423f5350-30f1-4a6b-8b1c-1a76206690b4",
   "metadata": {},
   "source": [
    "## Task-1: Compute the gradient of BCE with respect to the parameters (w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b02bb4e9-483b-4667-999d-c1fd5a48c1c2",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b47fecca5757da8a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to compute the gradient of MSE with respect to the parameters (w, b)\n",
    "def compute_gradients(X, y_true, y_pred):\n",
    "    \n",
    "    d_pred_loss = bce_derivative(y_true, y_pred)             # Derivative of loss w.r.t y_pred\n",
    "    d_activation_loss = sigmoid_derivative(y_pred)           # Derivative of sigmoid activation\n",
    "    dL_dz = d_pred_loss * d_activation_loss                  # Chain rule: derivative w.r.t z\n",
    "    dL_dw = np.dot(X.T, dL_dz) / X.shape[0]                  # Gradient w.r.t weights\n",
    "    dL_db = np.mean(dL_dz, axis=0)                           # Gradient w.r.t bias\n",
    "    \n",
    "    return dL_dw, dL_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de427f3e-3b2f-4ccd-aa4e-a34626bfbcc8",
   "metadata": {},
   "source": [
    "## Task-2: Update parameters W (weights) and b (bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a25a1ab8-1c69-4716-9a6a-6139a4c48f16",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-347477305ecd434f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(W, X, b, num_epochs):\n",
    "    \n",
    "    learning_rate=0.01\n",
    "    loss = 0\n",
    "   \n",
    "    for epoch in range(0, num_epochs):\n",
    "        y_pred = forward_propagation(X, W, b)\n",
    "        loss = compute_bce(y_true, y_pred)\n",
    "       \n",
    "        # Compute the gradients\n",
    "        dw, db = compute_gradients(X, y_true, y_pred)\n",
    "       \n",
    "        # update parameters W (weights) and b (bias)\n",
    "        W = W - dw * learning_rate\n",
    "        b = b - db * learning_rate\n",
    "\n",
    "        print(f\"Epoch {epoch+1}    loss = {round(loss, 4)}\")\n",
    "    return loss   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "502ef046-c952-416b-86e4-528ca2cc151b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1    loss = 1.5343\n",
      "Epoch 2    loss = 1.5208\n",
      "Epoch 3    loss = 1.5077\n",
      "Epoch 4    loss = 1.495\n",
      "Epoch 5    loss = 1.4827\n",
      " final loss = 1.4827\n"
     ]
    }
   ],
   "source": [
    "final_loss = train(W, X, b, 5)\n",
    "print(f\" final loss = {round(final_loss, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a51700-9881-4261-bba8-8aa68a56e976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
