{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982428f4-693e-419f-a684-0d7473a4eed5",
   "metadata": {},
   "source": [
    "# Demonstrating Scaled Dot-Product Attention & Self-Attention in Transformers\n",
    "## This notebook will provide an intuitive and practical demonstration of Scaled Dot-Product Attention and Self-Attention in Transformers using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621a7154-1bbc-410d-85a4-e7622ce3cf79",
   "metadata": {},
   "source": [
    "![Description](Scaled_Dot_Product_Attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a78fc9-b9b2-4c05-8766-17b41a788e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "907c6e65-825b-474e-a62f-7eaaec13be2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f691466f970>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)  # Python random seed\n",
    "torch.manual_seed(42)  # PyTorch seed (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d71ba22-cc40-4587-880f-19747e034698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set print options: No scientific notation, 2 decimal places\n",
    "torch.set_printoptions(sci_mode=False, precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c089c-b5ba-4323-bedd-c37f957a44fb",
   "metadata": {},
   "source": [
    "# Define the maximum sequence length and the embedding dimension for a model:\n",
    "\n",
    "max_sequence_length = 10: Specifies the maximum number of tokens a sequence can have. If a sequence is shorter, it may be padded; if longer, it may be truncated.\n",
    "\n",
    "d_model = 8: Defines the size of each token’s embedding vector, meaning each token will be represented as a     8-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b41bf5e-0789-4d55-984a-e40711ffaa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 8\n",
    "max_sequence_length = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779cd4ac-8435-4b94-a88a-6db4d75d81d6",
   "metadata": {},
   "source": [
    "# Define three linear layers using nn.Linear in PyTorch:\n",
    "\n",
    "w_query: Projects input embeddings into query space.\n",
    "\n",
    "w_key: Projects input embeddings into key space.\n",
    "\n",
    "w_value: Projects input embeddings into value space.\n",
    "## These linear layers transform input embeddings (d_model dimensional) into new representations of the same size (d_model → d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc607257-8149-4d14-b3b0-17f2f1d9bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_query = nn.Linear(d_model, d_model)\n",
    "w_key   = nn.Linear(d_model, d_model)\n",
    "w_value = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9a6f22-919f-4ca8-a9cb-ec2e731ebbff",
   "metadata": {},
   "source": [
    "# w_query, has a learnable weight matrix of shape (d_model, d_model).\n",
    "\n",
    "## The .weight attribute stores the trainable parameters of this transformation.\n",
    "\n",
    "### Display the randomly initialized values of w_query.weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa2a1d97-9238-45b8-b00e-dba8f9e33833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2703,  0.2935, -0.0828,  0.3248, -0.0775,  0.0713, -0.1721,  0.2076],\n",
       "        [ 0.3117, -0.2594,  0.3073,  0.0662,  0.2612,  0.0479,  0.1705, -0.0499],\n",
       "        [ 0.2725,  0.0523, -0.1651,  0.0901, -0.1629, -0.0415, -0.1436,  0.2345],\n",
       "        [-0.2791, -0.1630, -0.0998, -0.2126,  0.0334, -0.3492,  0.3193, -0.3003],\n",
       "        [ 0.2730,  0.0588, -0.1148,  0.2185,  0.0551,  0.2857,  0.0387, -0.1115],\n",
       "        [ 0.0950, -0.0959,  0.1488,  0.3157,  0.2044, -0.1546,  0.2041,  0.0633],\n",
       "        [ 0.1795, -0.2155, -0.3500, -0.1366, -0.2712,  0.2901,  0.1018,  0.1464],\n",
       "        [ 0.1118, -0.0062,  0.2767, -0.2512,  0.0223, -0.2413,  0.1090, -0.1218]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_query.weight # , w_key.weight, w_value.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf1f29-ff7c-405c-9ee1-69cb897c2c7d",
   "metadata": {},
   "source": [
    "# Create a tensor tokens with random values, scaled by a factor of 10.0, to simulate a sequence of token embeddings.\n",
    "\n",
    "Use torch.randn() to generate a random tensor of shape (max_sequence_length, d_model), where:\n",
    "\n",
    "max_sequence_length represents the number of tokens in the sequence.\n",
    "\n",
    "d_model represents the embedding dimension.\n",
    "\n",
    "Multiply the generated tensor by 10.0 to scale the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a599c7e-007f-43da-b48c-bf65e0a3bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.randn(max_sequence_length, d_model) * 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1247bcd9-9682-4329-80c6-d30fa9f31596",
   "metadata": {},
   "source": [
    "# Print the shape of the tensor tokens to confirm its dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27cb32f2-fec9-4220-ac49-2009e9d5b6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c6c050-a11f-4a81-86ea-2cae49b9daf2",
   "metadata": {},
   "source": [
    "# Print the tokens tensor to inspect its values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c394140-2545-4b1d-8dda-2a42059ae728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    13.0321,      4.8787,     11.3399,     -3.5556,      3.6183,\n",
       "             19.9935,      6.6301,      7.0473],\n",
       "        [     0.2127,     -8.2927,    -10.8086,     -7.8385,      5.0710,\n",
       "              0.8208,      4.4398,     -7.2403],\n",
       "        [    -4.6113,     -0.6388,    -13.6673,      3.2982,     -9.8271,\n",
       "              3.0177,      1.7869,     -1.2931],\n",
       "        [   -15.7541,     22.5084,     10.0123,     13.6424,      6.3332,\n",
       "              4.0500,      3.4159,     -2.2136],\n",
       "        [     1.7290,     10.5136,      0.0749,     -0.7737,      6.4269,\n",
       "              5.7425,      5.8672,     -0.1885],\n",
       "        [    -9.1432,     14.8397,     -9.1091,     -5.2910,     -8.0515,\n",
       "              5.1580,     -7.1288,      2.1962],\n",
       "        [     5.6351,     18.5822,     10.4407,     -8.6382,      8.3509,\n",
       "             -3.1571,      2.6911,      0.8540],\n",
       "        [   -14.1288,    -18.7906,     -1.7983,      7.9039,      5.2394,\n",
       "             -2.6935,    -16.1906,      0.0126],\n",
       "        [     8.6374,     -5.8900,    -10.3400,     -2.1787,      7.9865,\n",
       "              9.1047,     -0.8802,      3.3700],\n",
       "        [    -4.8076,      3.1630,      3.8657,      7.3369,      2.5103,\n",
       "              0.7700,     -2.0634,     21.6979]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1939bb0-6cd8-4faf-85bb-920de1c559ae",
   "metadata": {},
   "source": [
    "# Apply linear transformations to the tokens tensor using w_query, w_key, and w_value to obtain query (q), key (k), and value (v) representations.\n",
    "\n",
    "## Pass tokens through the three linear layers to compute q, k, and v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00475ad2-51a4-4462-8d1a-976b5f04eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = w_query(tokens)\n",
    "k = w_key(tokens)\n",
    "v = w_value(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a27926-cb7f-4ca4-889c-a81b38d94630",
   "metadata": {},
   "source": [
    "# Print the shapes of q, k, and v to verify their dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a36b478-8a8a-4407-a5f1-83462a95d20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 8]), torch.Size([10, 8]), torch.Size([10, 8]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1ba828-35d6-481d-a798-4c8f1d4c95b8",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "\n",
    "$$\n",
    "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{new V} = \\text{self attention}.V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ffdc7-cdb2-4ad5-b6b8-d7a425e09911",
   "metadata": {},
   "source": [
    "# Compute the attention score matrix using torch.matmul(q, k.T) without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84611b03-0c27-454c-a05e-d30a93069840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-108.3274,   57.7419,   18.5742,  -58.1449,  -47.4800,  -68.6371,\n",
       "          -77.5921,   66.9448,  -33.8048,   -2.1428],\n",
       "        [ -34.7808,   12.9728,   -8.8561,   -5.0074,  -12.5370,    3.0406,\n",
       "           28.6565,  -22.4715,  -53.5321, -107.4938],\n",
       "        [ -36.0648,   17.3945,   62.1235,  -34.9770,    0.7812,  106.7213,\n",
       "            4.0961,  -25.0537,   13.3692,    5.9479],\n",
       "        [  55.7433,  -23.3669,   33.5451,  -25.7747,   25.1168,  -13.1104,\n",
       "          -33.6642,  -43.2133,   85.4181,  161.3757],\n",
       "        [ -13.6088,   20.2927,   24.7524,  -56.7094,  -12.7326,  -14.9126,\n",
       "          -35.0543,  -10.1397,   15.2518,   15.7781],\n",
       "        [ 104.5473,  -97.4240,   57.7952,  133.3300,   67.0595,  168.0302,\n",
       "           56.6170,  -78.5524,   29.1397,  182.7953],\n",
       "        [ 121.6384,  -55.9330,  -30.9015,   32.2752,   19.1241,  -50.3688,\n",
       "           -1.5578,  -33.8538,   20.8590,   58.9215],\n",
       "        [ -19.7858,  -32.7079,  -34.7863,  140.5346,   35.8255,   17.4498,\n",
       "           59.9764,   32.0895,  -20.5953,   46.9739],\n",
       "        [-119.9982,   72.3046,   36.9589,  -65.6181,  -35.4025,    8.3656,\n",
       "          -41.6900,   46.2909,  -22.6066,  -46.5447],\n",
       "        [ -26.6542,    4.7384,    7.3613,   22.5161,    7.6967,   22.0567,\n",
       "          -20.6234,   72.5867,   45.5824,   97.1520]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(q, k.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a3aa4b-b922-4c72-93ea-2bdb14289be0",
   "metadata": {},
   "source": [
    "# Observe that the variance of the dot product is significantly larger than the variance of q and k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31158941-4fc0-4dc3-8290-104fa16bab67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(21.7614, grad_fn=<VarBackward0>),\n",
       " tensor(31.7007, grad_fn=<VarBackward0>),\n",
       " tensor(3491.7310, grad_fn=<VarBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why we need sqrt(d_k) in denominator\n",
    "q.var(), k.var(), torch.matmul(q, k.T).var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb9e52-670b-451c-b88e-821a704ed472",
   "metadata": {},
   "source": [
    "# Compute the attention scores by taking the dot product of query (q) and key (k.T) using torch.matmul(q, k.T).\n",
    "\n",
    "# Normalize the scores by dividing them by the square root of d_model to stabilize the variance and prevent extremely large values.\n",
    "\n",
    "## Store the result in attn_scores, which will be used for the softmax operation in the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a797931e-a1ea-4876-b5d9-35e55677692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = torch.matmul(q, k.T) / torch.sqrt(torch.tensor(d_model, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df944ddd-2191-40ee-b917-7ff13ab5f5f4",
   "metadata": {},
   "source": [
    "# observe how scaling affects variance, compare q.var(), k.var(), and attn_scores.var() after applying the scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44b1026f-8761-4107-be2f-29608767ad8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(21.7614, grad_fn=<VarBackward0>),\n",
       " tensor(31.7007, grad_fn=<VarBackward0>),\n",
       " tensor(436.4664, grad_fn=<VarBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why we need sqrt(d_k) in denominator\n",
    "q.var(), k.var(), attn_scores.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e770417e-2d8b-4b80-86e9-90d8cbe2b300",
   "metadata": {},
   "source": [
    "# Print the attention scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c7926e6-c9fc-41a2-be0b-91d8b5ae4709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-38.2995,  20.4149,   6.5670, -20.5573, -16.7867, -24.2669, -27.4329,\n",
       "          23.6686, -11.9518,  -0.7576],\n",
       "        [-12.2969,   4.5866,  -3.1311,  -1.7704,  -4.4325,   1.0750,  10.1316,\n",
       "          -7.9449, -18.9265, -38.0048],\n",
       "        [-12.7508,   6.1499,  21.9640, -12.3662,   0.2762,  37.7317,   1.4482,\n",
       "          -8.8578,   4.7267,   2.1029],\n",
       "        [ 19.7082,  -8.2615,  11.8600,  -9.1127,   8.8801,  -4.6352, -11.9021,\n",
       "         -15.2782,  30.1999,  57.0549],\n",
       "        [ -4.8114,   7.1746,   8.7513, -20.0498,  -4.5017,  -5.2724, -12.3936,\n",
       "          -3.5849,   5.3923,   5.5784],\n",
       "        [ 36.9631, -34.4446,  20.4337,  47.1393,  23.7091,  59.4077,  20.0171,\n",
       "         -27.7725,  10.3025,  64.6279],\n",
       "        [ 43.0057, -19.7753, -10.9253,  11.4110,   6.7614, -17.8080,  -0.5508,\n",
       "         -11.9691,   7.3748,  20.8319],\n",
       "        [ -6.9953, -11.5640, -12.2988,  49.6865,  12.6662,   6.1695,  21.2048,\n",
       "          11.3453,  -7.2815,  16.6078],\n",
       "        [-42.4258,  25.5635,  13.0669, -23.1995, -12.5167,   2.9577, -14.7396,\n",
       "          16.3663,  -7.9926, -16.4560],\n",
       "        [ -9.4237,   1.6753,   2.6026,   7.9607,   2.7212,   7.7982,  -7.2915,\n",
       "          25.6633,  16.1158,  34.3484]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ab8e6-1d5f-42ce-bc3c-de85005c61c5",
   "metadata": {},
   "source": [
    "# Print the shape of the attention scores to understand their dimensions\n",
    "## The shape of attn_scores should be (max_sequence_length, max_sequence_length), indicating that each token attends to every other token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcad2e3b-8953-4f3c-a00c-41933c948b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002b3c6a-3dbe-4b16-b2bf-90906b75907d",
   "metadata": {},
   "source": [
    "# Apply the softmax function to the attention scores to obtain attention weights\n",
    "## This converts the raw attention scores into probabilities, ensuring that the sum of attention weights across each row equals 1.\n",
    "\n",
    "## Higher values indicate stronger attention to specific tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d85b218a-5c9b-4c02-b867-d88ba84882ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = F.softmax(attn_scores, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6677169b-179d-4231-b581-e9dae36cc5f4",
   "metadata": {},
   "source": [
    "# Print the attention weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da66f370-864b-448d-8721-41e54819d00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0.0000,     0.0372,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "             0.0000,     0.9628,     0.0000,     0.0000],\n",
       "        [    0.0000,     0.0039,     0.0000,     0.0000,     0.0000,     0.0001,\n",
       "             0.9960,     0.0000,     0.0000,     0.0000],\n",
       "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     1.0000,\n",
       "             0.0000,     0.0000,     0.0000,     0.0000],\n",
       "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "             0.0000,     0.0000,     0.0000,     1.0000],\n",
       "        [    0.0000,     0.1610,     0.7792,     0.0000,     0.0000,     0.0000,\n",
       "             0.0000,     0.0000,     0.0271,     0.0326],\n",
       "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0054,\n",
       "             0.0000,     0.0000,     0.0000,     0.9946],\n",
       "        [    1.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "             0.0000,     0.0000,     0.0000,     0.0000],\n",
       "        [    0.0000,     0.0000,     0.0000,     1.0000,     0.0000,     0.0000,\n",
       "             0.0000,     0.0000,     0.0000,     0.0000],\n",
       "        [    0.0000,     0.9999,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "             0.0000,     0.0001,     0.0000,     0.0000],\n",
       "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "             0.0000,     0.0002,     0.0000,     0.9998]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdaf4c2-c5f7-4eae-b2da-b08eeb7736c7",
   "metadata": {},
   "source": [
    "# Compute the weighted sum of value (v) vectors using the attention weights\n",
    "Each query now receives a context-aware representation by combining value vectors (v), where the contribution of each value is determined by the attention weights. This helps in focusing on the most relevant parts of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "955815d4-ca60-443f-9527-a3e43969b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output = torch.matmul(attn_weights, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafecd84-02ee-444e-bc06-1c3aaf9b1998",
   "metadata": {},
   "source": [
    "# Print the shape of the attention output to understand its dimensions\n",
    "The shape of attention_output should match the input shape of v, confirming that each token has been transformed into a new representation based on attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aabe3469-53d5-4bd4-bb6b-e5c8950cff8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e831c2e-4b70-46e3-b7f1-e4bfc5275bb1",
   "metadata": {},
   "source": [
    "# Print the attention output to observe how the values have been transformed after applying attention weights\n",
    "This output represents the refined token representations after the attention mechanism has weighted and aggregated the value vectors (v) based on the computed attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98ed7b90-4efa-4faa-a3a2-d41c2a389bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2.1261,      1.8769,     -4.6177,      7.3562,     -3.0561,\n",
       "             -6.4733,    -10.4790,     11.3530],\n",
       "        [     4.0124,      1.9254,      7.8426,     -3.4303,      6.4231,\n",
       "              5.0072,      7.5677,     -2.2979],\n",
       "        [    -6.6414,     -7.1332,      0.0274,     -2.6242,      2.7893,\n",
       "              1.1445,      2.5857,     -2.2104],\n",
       "        [     0.3210,     -5.0049,      4.0713,     -3.4348,     -2.5455,\n",
       "             -2.8200,      1.0738,     -2.9991],\n",
       "        [    -4.3984,     -4.0713,     -3.9356,      0.6972,     -3.2912,\n",
       "             -2.2215,     -1.1523,     -2.2966],\n",
       "        [     0.2835,     -5.0163,      4.0495,     -3.4305,     -2.5169,\n",
       "             -2.7986,      1.0819,     -2.9949],\n",
       "        [    -4.4170,      5.4806,      6.9589,     -5.0158,      4.6338,\n",
       "              4.0796,      5.1147,     -9.5842],\n",
       "        [     9.7750,     -3.0269,     -7.3841,     -7.0403,      6.6433,\n",
       "              1.1184,      1.8107,      6.3116],\n",
       "        [    -2.0511,      3.8129,     -0.6091,      4.7768,     -0.0010,\n",
       "             -2.6146,     -1.8607,     -1.8721],\n",
       "        [     0.3213,     -5.0037,      4.0698,     -3.4330,     -2.5456,\n",
       "             -2.8206,      1.0718,     -2.9966]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
