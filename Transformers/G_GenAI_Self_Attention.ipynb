{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982428f4-693e-419f-a684-0d7473a4eed5",
   "metadata": {},
   "source": [
    "# Demonstrating Scaled Dot-Product Attention & Self-Attention in Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621a7154-1bbc-410d-85a4-e7622ce3cf79",
   "metadata": {},
   "source": [
    "![Description](Scaled_Dot_Product_Attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a78fc9-b9b2-4c05-8766-17b41a788e1f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f49567c7a5857081",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "907c6e65-825b-474e-a62f-7eaaec13be2d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7f6a2fa21e082f19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fed7f73f970>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)  # Python random seed\n",
    "torch.manual_seed(42)  # PyTorch seed (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d71ba22-cc40-4587-880f-19747e034698",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0c122e28d9c64d4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set print options: No scientific notation, 2 decimal places\n",
    "torch.set_printoptions(sci_mode=False, precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c71312-2bba-4e7c-b7b5-2582d7f342ee",
   "metadata": {},
   "source": [
    "# Task : Implementing Self-Attention\n",
    "In this task, you will implement a Self-Attention mechanism, a fundamental building block of the Transformer model. Self-attention allows a model to weigh the importance of different words in a sequence when encoding contextual information.\n",
    "\n",
    "Your goal is to define a SelfAttention class in PyTorch, which will:\n",
    "\n",
    "Initialize Linear Layers: Learnable transformations for query (Q), key (K), and value (V) projections.\n",
    "\n",
    "Compute Attention Scores: Measure how much focus each word should give to others using dot-product attention.\n",
    "\n",
    "Apply Softmax Scaling: Normalize attention scores to get attention weights.\n",
    "\n",
    "Generate Contextual Output: Multiply attention weights with value vectors to obtain the final representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c10cbaa8-d0c5-45fd-b5f8-1baa4282a509",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7dcca7eaaa9a1adb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model\n",
    "        self.d_v = d_model\n",
    "\n",
    "        ### BEGIN SOLUTION\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        ### END SOLUTION   \n",
    "        \n",
    "    def forward(self, q_input, k_input, v_input):\n",
    "\n",
    "        Q = self.query(q_input)\n",
    "        K = self.key(k_input)\n",
    "        V = self.value(v_input)\n",
    "        \n",
    "        ### BEGIN SOLUTION\n",
    "        attn_scores = torch.matmul(Q, K.T) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attention_output = torch.matmul(attn_weights, V)\n",
    "        ### END SOLUTION   \n",
    "        \n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2572286e-bdf7-4fb9-b4d6-8431523d564e",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6c485a8dd96d14d9",
     "locked": true,
     "points": 100,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def test_self_attention():\n",
    "    d_model_t = 16   # Feature dimension\n",
    "    seq_len_t = 2    # Sequence length\n",
    "\n",
    "    # Initialize the SelfAttention module\n",
    "    self_attn_t = SelfAttention(d_model_t)\n",
    "\n",
    "    # Create random inputs of shape (seq_len, d_model)\n",
    "    q_input_t = torch.randn(seq_len_t, d_model_t)\n",
    "    k_input_t = torch.randn(seq_len_t, d_model_t)\n",
    "    v_input_t = torch.randn(seq_len_t, d_model_t)\n",
    "\n",
    "    # Forward pass\n",
    "    attention_output_t = self_attn_t(q_input_t, k_input_t, v_input_t)\n",
    "\n",
    "    # Assertions\n",
    "    assert attention_output_t.shape == (seq_len_t, d_model_t), \"Output shape mismatch!\"\n",
    "    assert not torch.isnan(attention_output_t).any(), \"NaN values in output!\"\n",
    "    assert torch.isfinite(attention_output_t).all(), \"Non-finite values in output!\"\n",
    "\n",
    "    print(\"All test cases passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_self_attention()\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d17dc665-774d-4300-bc75-6013ea76d6f0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6194b76748e8218",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "d_model=6\n",
    "max_sequence_length = 4\n",
    "src_tokens = torch.randn(max_sequence_length, d_model) * 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f5dbd2b-fde1-44cd-99de-b420317850ce",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-422b56c455e6087f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "self_attention = SelfAttention(d_model)\n",
    "result = self_attention.forward(src_tokens, src_tokens, src_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "161db305-8a7b-42cc-988d-ddeb2533a015",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-041dd90593bfa659",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -2.4043,  -1.2668,   3.1681,   5.2007,  -4.8781,  -3.4479],\n",
       "        [ -1.9338,   3.2367,  -1.3393,  -3.8062,  -0.8477,   4.8810],\n",
       "        [ -2.4011,  -1.2671,   3.1729,   5.2080,  -4.8893,  -3.4525],\n",
       "        [ -0.8060,  -1.1678,   5.2952,   8.3120, -10.1812,  -5.2827]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c61a4b5-c85f-494b-ace4-ce13fdf16f24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
