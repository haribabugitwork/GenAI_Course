{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72419184-0ee2-4477-ba50-7bd4e6458000",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5cd3e8ba5f45441c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0371393c-fac6-4d61-af51-c99506400bc6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-94bb9e10a1b524e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f96343a38d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(24)  # Python random seed\n",
    "torch.manual_seed(24)  # PyTorch seed (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8f74e8a-d61e-4b4a-958c-747d5dcd85f8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c78aa560595c8b3f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set print options: No scientific notation, 2 decimal places\n",
    "torch.set_printoptions(sci_mode=False, precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bc0967-4e42-4908-94d9-a12fb159e0b3",
   "metadata": {},
   "source": [
    "# Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03aacfc8-1659-4256-b61a-3127d525a6a8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e1bac5426e9857bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Tokenizer and Vocabulary Creation (Add more words if needed)\n",
    "sentence_en = \"I love Dhoni .\"\n",
    "# sentence_fr = \"J' adore l'IA .\"\n",
    "sentence_te = \"నాకు ధోని అంటే ఇష్టం .\"\n",
    "\n",
    "word_map_en = {\"<pad>\": 0, \"I\": 1, \"love\": 2, \"Dhoni\": 3, \".\": 4}\n",
    "word_map_te = {\"<pad>\": 0, \"నాకు\": 1, \"ధోని\": 2, \"అంటే\": 3, \"ఇష్టం\": 4, \".\": 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15935042-ab1f-4a5f-abc2-e64673bd2446",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2fb247f8d1fde15a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizing sentences\n",
    "def tokenize(sentence, word_map):\n",
    "    return torch.tensor([word_map[word] for word in sentence.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87a89eca-f6f2-448b-bded-4c3c83819518",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-24daef2b08eae1c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the input and target sentences\n",
    "input_tensor = tokenize(sentence_en, word_map_en).unsqueeze(0)  # Shape (1, 4)\n",
    "target_tensor = tokenize(sentence_te, word_map_te).unsqueeze(0)  # Shape (1, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec97d01-6746-4a97-a90d-fa2950671834",
   "metadata": {},
   "source": [
    "# Task: Implement the PositionalEncoding class"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60447b87-f096-4157-800d-b9b3be23ca90",
   "metadata": {},
   "source": [
    "You are tasked with implementing a Positional Encoding layer in PyTorch, which will be used to inject information about the position of tokens in a sequence. This encoding is crucial for models like the Transformer, which lacks any inherent notion of token position in its architecture.\n",
    "1.Positional Encoding Initialization:\n",
    "\n",
    "The PositionalEncoding class will be initialized with the following parameters:\n",
    "d_model: The dimension of the embeddings (e.g., size of word vectors).\n",
    "max_len: The maximum sequence length that can be processed (default is 5000).\n",
    "The positional encoding should be created based on the formula using sine and cosine functions.\n",
    "\n",
    "2.Encoding Calculation:\n",
    "\n",
    "You need to calculate the positional encoding matrix using the suitable formula for even indices and odd indices.\n",
    "\n",
    "This positional encoding matrix will be added to the input sequence in the forward method.\n",
    "\n",
    "3.Forward Pass:\n",
    "\n",
    "In the forward method, the input tensor x will be added to the positional encoding corresponding to its sequence length.\n",
    "The output tensor will be the sum of the input tensor and the positional encoding, which gives the input tokens with position information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c45ee-9276-4950-8dd4-fab6131ee5a3",
   "metadata": {},
   "source": [
    "## Define the PositionalEncoding Class\n",
    "Create a class that inherits from nn.Module.\n",
    "\n",
    "Initialize the embedding dimension (d_model) and the maximum sequence length (max_sequence_length).\n",
    "\n",
    "## Initialize the Positional Encoding Matrix\n",
    "Create a tensor of shape (max_sequence_length, d_model), filled with zeros.\n",
    "\n",
    "This will store the sinusoidal positional encodings.\n",
    "\n",
    "## Compute the Denominators for Sine and Cosine Functions\n",
    "Generate indices i for half the embedding dimension (d_model//2).\n",
    "\n",
    "Compute the denominator using the formula:\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\bigg(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\bigg(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\bigg)\n",
    "$$\n",
    "\n",
    "## Generate Token Positions\n",
    "Create a column vector of token positions (token_pos) with shape (max_sequence_length, 1).\n",
    "\n",
    "## Apply Sine and Cosine Functions\n",
    "Sine values are assigned to even indices (0, 2, 4, ...).\n",
    "\n",
    "Cosine values are assigned to odd indices (1, 3, 5, ...).\n",
    "\n",
    "## Add a Batch Dimension\n",
    "Reshape the positional encoding to support batch processing.\n",
    "\n",
    "## Implement the forward Method\n",
    "Extract the relevant positional encodings for the input sequence (x).\n",
    "\n",
    "Add the positional encodings to the input sequence (x).\n",
    "\n",
    "Return the modified input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c9d91d-1eb3-429e-b730-f701d6ae053f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-257ca44b8124d463",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        ### BEGIN SOLUTION\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "        self.PE = torch.zeros(max_sequence_length, d_model)\n",
    "\n",
    "        i = torch.arange(0, self.d_model//2).float()\n",
    "        denominator = torch.pow(10000, 2*i/self.d_model)\n",
    "        token_pos = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
    "        self.PE[:, 0::2] = torch.sin(token_pos / denominator)\n",
    "        self.PE[:, 1::2] = torch.cos(token_pos / denominator)\n",
    "        \n",
    "        self.PE = self.PE.unsqueeze(0)  # Add batch dimension\n",
    "        ### END SOLUTION\n",
    "    def forward(self, x):\n",
    "        ### BEGIN SOLUTION\n",
    "        pos_enc = self.PE[:, :x.size(1)]\n",
    "        # print(\"\\nPositional Encoding Values:\")\n",
    "        # print(pos_enc)\n",
    "\n",
    "        final_output = x + pos_enc\n",
    "        # print(\"\\nFinal Input After Adding Positional Encoding:\")\n",
    "        # print(final_output)\n",
    "        ### END SOLUTION\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25b2cd5b-bc87-4826-b08a-aa27602cb651",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0a7575a3789cc903",
     "locked": true,
     "points": 80,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed successfully! \n"
     ]
    }
   ],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "def test_positional_encoding():\n",
    "    d_model = 6\n",
    "    max_sequence_length = 10\n",
    "    pos_enc_t = PositionalEncoding(d_model, max_sequence_length)\n",
    "\n",
    "    # Test 1: Shape correctness\n",
    "    x = torch.randn(1, max_sequence_length, d_model)\n",
    "    output = pos_enc_t(x)\n",
    "    assert output.shape == x.shape, \"Test 1 Failed: Output shape mismatch!\"\n",
    "\n",
    "    # Test 2: Ensure positional encoding is added\n",
    "    x_zero = torch.zeros(1, max_sequence_length, d_model)  # Zero input tensor\n",
    "    output_zero = pos_enc_t(x_zero)\n",
    "    assert not torch.allclose(output_zero, x_zero), \"Test 2 Failed: Positional encoding was not added!\"\n",
    "\n",
    "    # Test 3: Batch processing\n",
    "    x_batch = torch.randn(2, max_sequence_length, d_model)  # Batch of size 2\n",
    "    output_batch = pos_enc_t(x_batch)\n",
    "    assert output_batch.shape == x_batch.shape, \"Test 3 Failed: Batch processing failed!\"\n",
    "\n",
    "    # Test 4: Handling partial sequences\n",
    "    x_short = torch.randn(1, 6, d_model)  # Sequence shorter than max length\n",
    "    output_short = pos_enc_t(x_short)\n",
    "    assert output_short.shape == x_short.shape, \"Test 4 Failed: Positional encoding does not support shorter sequences!\"\n",
    "\n",
    "    print(\"All tests passed successfully! \")\n",
    "\n",
    "# Run the test\n",
    "test_positional_encoding()\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e5923-6c20-496a-93e7-da5878e9e947",
   "metadata": {},
   "source": [
    "# Task: Implement a Transformer model that combines word embeddings with positional encoding to capture both word meaning and order in a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9671b4e0-9199-4480-93f6-20fd59448756",
   "metadata": {},
   "source": [
    "## Initialize Embedding Layer\n",
    "\n",
    "Use nn.Embedding to map words (tokens) to dense vector representations of size d_model.\n",
    "\n",
    "## Integrate Positional Encoding\n",
    "\n",
    "create a PositionalEncoding instance to incorporate position information.\n",
    "\n",
    "## Define the Forward Pass\n",
    "\n",
    "Convert input tokens into embeddings using self.embedding(src).\n",
    "\n",
    "Pass the embeddings through self.pos_encoder to add positional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99aa9a61-158c-4652-88af-b626cfa6cc1f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-49cb7810c93b8bd7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len=10):\n",
    "        super(Transformer, self).__init__()\n",
    "        ### BEGIN SOLUTION\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "        ### END SOLUTION\n",
    "    def forward(self, src):\n",
    "        ### BEGIN SOLUTION\n",
    "        src_embedding = self.embedding(src)\n",
    "        # print(f\"\\nSrc Embedding Values: {src_embedding}\")\n",
    "        input_embedding = self.pos_encoder(src_embedding)\n",
    "        ### END SOLUTION\n",
    "        return input_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fb27374-2e8b-4d99-8248-d7afb8b5e359",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-61bbdd86aef4a85d",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "def test_transformer():\n",
    "    vocab_size = 20  # Example vocabulary size\n",
    "    d_model = 6  # Embedding dimension\n",
    "    max_len = 10  # Maximum sequence length\n",
    "    \n",
    "    transformer_t = Transformer(vocab_size, d_model, max_len)  # Instance renamed to transformer_t\n",
    "\n",
    "    # Test case 1: Single sequence of token indices\n",
    "    input_tensor_1 = torch.tensor([[1, 2, 3, 4, 5]])  # A sample input sequence\n",
    "    output_1 = transformer_t(input_tensor_1)\n",
    "    assert output_1.shape == (1, 5, d_model), \"Test Case 1 Failed: Output shape mismatch\"\n",
    "\n",
    "    # Test case 2: Multiple sequences (batch processing)\n",
    "    input_tensor_2 = torch.tensor([[3, 4, 5, 6, 7], [2, 3, 4, 5, 6]])\n",
    "    output_2 = transformer_t(input_tensor_2)\n",
    "    assert output_2.shape == (2, 5, d_model), \"Test Case 2 Failed: Output shape mismatch\"\n",
    "\n",
    "    # Test case 3: Edge case - using the highest index in vocabulary\n",
    "    input_tensor_3 = torch.tensor([[vocab_size - 1, vocab_size - 2, vocab_size - 3]])\n",
    "    output_3 = transformer_t(input_tensor_3)\n",
    "    assert output_3.shape == (1, 3, d_model), \"Test Case 3 Failed: Output shape mismatch\"\n",
    "\n",
    "    print(\"All test cases passed!\")\n",
    "\n",
    "# Run the test function\n",
    "test_transformer()\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cb1744d-4c03-4c72-bf24-19417cf0092e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-095d0bdb349f0592",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#  Initialize Model\n",
    "vocab_size_en = len(word_map_en)\n",
    "vocab_size_te = len(word_map_te)\n",
    "d_model = 8\n",
    "max_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebedd494-5298-47ba-b970-66d14612ad1d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee1bd23df9304dcf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(vocab_size_en, d_model, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c835e81-56ba-4f21-8a12-14f3e038be4f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3101ab64f3ca4a5a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# transformer\n",
    "output = transformer(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c824486-49ae-4bd5-9b08-d56d8f34a3be",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e9e7b8700640dbbb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7510, -1.0548,  0.1096, -0.5704,  1.9079, -0.6084, -1.2071,\n",
       "           1.0335],\n",
       "         [ 0.1277,  0.1987, -0.0825, -0.2313, -0.9619,  1.6389,  0.1670,\n",
       "           2.0120],\n",
       "         [ 1.3652,  0.1752,  1.2235,  1.0922,  2.6103,  1.8005, -1.2510,\n",
       "           1.1712],\n",
       "         [ 0.7820, -0.5253,  0.3549,  1.3561,  1.0577,  2.1537, -0.0899,\n",
       "           1.3271]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098da658-6864-48de-aec5-1a4ee76f3eda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
