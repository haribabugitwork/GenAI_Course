{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982428f4-693e-419f-a684-0d7473a4eed5",
   "metadata": {},
   "source": [
    "# Demonstrating Masked Self-Attention in Transformers\n",
    "## This notebook will provide an intuitive and practical demonstration of Masked Self-Attention in Transformers using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53eac97-2583-4e8b-8506-01b837f8834f",
   "metadata": {},
   "source": [
    "## Masked Self Attention\n",
    "\n",
    "$$\n",
    "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{new V} = \\text{self attention}.V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a78fc9-b9b2-4c05-8766-17b41a788e1f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dbc97a4d54009a8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73a09000-1d97-4fb7-a167-c9a095c39460",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-34063b942d51aed2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa4fb7038f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(24)  # Python random seed\n",
    "torch.manual_seed(24)  # PyTorch seed (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dabcfc2-b41c-436b-8377-c8ebfb4c4206",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea73aa8aa9b73496",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set print options: No scientific notation, 2 decimal places\n",
    "torch.set_printoptions(sci_mode=False, precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d2c80-94da-4b07-9860-0e3b7f94ee84",
   "metadata": {},
   "source": [
    "# Task: Implementing Masked Self-Attention\n",
    "In this task, you will implement a Masked Self-Attention mechanism, an essential component of Transformer-based decoders. Unlike regular self-attention, masked self-attention ensures that each token can only attend to itself and previous tokens, preventing access to future information during autoregressive generation.\n",
    "\n",
    "Your goal is to define a Masked_SelfAttention class in PyTorch, which will:\n",
    "\n",
    "Define learnable transformations for query (Q), key (K), and value (V) projections using nn.Linear.\n",
    "\n",
    "Measure the similarity between tokens using dot-product attention:\n",
    "\n",
    "Use a lower triangular mask to prevent attending to future tokens by setting masked positions to -inf.\n",
    "\n",
    "Convert masked attention scores into probability distributions using the softmax function.\n",
    "\n",
    "Multiply attention weights with value (V) vectors to produce the final contextual representation for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c10cbaa8-d0c5-45fd-b5f8-1baa4282a509",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-11470fa91892b68a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Masked_SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Masked_SelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model\n",
    "        self.d_v = d_model\n",
    "\n",
    "        ### BEGIN SOLUTION\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        ### END SOLUTION \n",
    "        \n",
    "    def forward(self, q_input, k_input, v_input, mask=None):\n",
    "\n",
    "        Q = self.query(q_input)\n",
    "        K = self.key(k_input)\n",
    "        V = self.value(v_input)\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.T) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float))\n",
    "       \n",
    "        if mask is not None:\n",
    "            ### BEGIN SOLUTION\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "            ### END SOLUTION \n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        ### BEGIN SOLUTION\n",
    "        attention_output = torch.matmul(attn_weights, V)\n",
    "        ### END SOLUTION \n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a97b6b5-e1bc-4aff-aaf2-ac69295bc238",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ec7b93350cb9a593",
     "locked": true,
     "points": 100,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "def test_masked_self_attention():\n",
    "    d_model_t = 16   # Feature dimension\n",
    "    seq_len_t = 4    # Sequence length\n",
    "\n",
    "    # Initialize the Masked Self-Attention module\n",
    "    masked_self_attn_t = Masked_SelfAttention(d_model_t)\n",
    "\n",
    "    # Create random inputs of shape (seq_len, d_model)\n",
    "    q_input_t = torch.randn(seq_len_t, d_model_t)\n",
    "    k_input_t = torch.randn(seq_len_t, d_model_t)\n",
    "    v_input_t = torch.randn(seq_len_t, d_model_t)\n",
    "\n",
    "    # Create a lower triangular mask for causal masking\n",
    "    mask_t = torch.tril(torch.ones(seq_len_t, seq_len_t))\n",
    "\n",
    "    # Forward pass\n",
    "    attention_output_t = masked_self_attn_t(q_input_t, k_input_t, v_input_t, mask_t)\n",
    "\n",
    "    # Assertions\n",
    "    assert attention_output_t.shape == (seq_len_t, d_model_t), \"Output shape mismatch!\"\n",
    "    assert not torch.isnan(attention_output_t).any(), \"NaN values in output!\"\n",
    "    assert torch.isfinite(attention_output_t).all(), \"Non-finite values in output!\"\n",
    "\n",
    "    print(\"All test cases passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_masked_self_attention()\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d17dc665-774d-4300-bc75-6013ea76d6f0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0b9f602fe728a929",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "d_model = 6\n",
    "max_sequence_length = 4\n",
    "src_tokens = torch.randn(max_sequence_length, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f5dbd2b-fde1-44cd-99de-b420317850ce",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cbc186cb352b2663",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "self_attention = Masked_SelfAttention(d_model)\n",
    "mask = torch.tril(torch.ones((max_sequence_length, max_sequence_length)))\n",
    "result = self_attention.forward(src_tokens, src_tokens, src_tokens, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "161db305-8a7b-42cc-988d-ddeb2533a015",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff82e7c6d5fef9f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6776,  0.0720, -0.4998, -0.3670,  0.1837,  0.3315],\n",
       "        [-0.6853,  0.2085, -0.2347, -0.4739,  0.0130,  0.5306],\n",
       "        [-0.3970, -0.0950,  0.1439, -0.1145,  0.0797,  0.4010],\n",
       "        [-0.3144, -0.3691,  0.4142, -0.0068, -0.0090,  0.1935]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f000b-4e90-400b-b143-c4674e3be5a5",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Attention a.k.a Cross-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "861b8125-1572-4b3c-850c-62258a4fcedd",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-82956e54d3b6f64d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Attention Output:\n",
      "tensor([[ 0.0314, -0.5712,  0.3103,  0.0950,  0.7929,  0.3417],\n",
      "        [ 0.0532, -0.5954,  0.3131,  0.0882,  0.7581,  0.3172],\n",
      "        [ 0.2182, -0.5214,  0.3097,  0.0350,  0.6196,  0.1594],\n",
      "        [-0.0774, -0.7373,  0.3256,  0.1355,  0.8371,  0.4414]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder_output = torch.randn(max_sequence_length, d_model)  # Simulated encoder output\n",
    "decoder_input = torch.randn(max_sequence_length, d_model)  # Simulated decoder input\n",
    "\n",
    "cross_attention = Masked_SelfAttention(d_model)\n",
    "result = cross_attention(decoder_input, encoder_output, encoder_output)\n",
    "\n",
    "print(\"Cross-Attention Output:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f92cda-92ed-497b-b26e-e2756843154e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
