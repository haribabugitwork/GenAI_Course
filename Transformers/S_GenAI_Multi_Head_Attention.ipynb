{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "588dbdcc-264b-4c62-911f-55b706e08df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79320154-1f2e-42de-a26f-bcd3d1dcdcb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb8b5e4f8f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(24)  # Python random seed\n",
    "np.random.seed(24)  # NumPy seed\n",
    "torch.manual_seed(24)  # PyTorch seed (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2745f2-4ca8-440f-8ea1-19f7637111f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set print options: No scientific notation, 2 decimal places\n",
    "torch.set_printoptions(sci_mode=False, precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e0cf65-434b-4fdc-95e4-458b3007c09d",
   "metadata": {},
   "source": [
    "# Define the maximum sequence length and the embedding dimension for a model:\n",
    "\n",
    "max_sequence_length = 5: Specifies the maximum number of tokens a sequence can have. If a sequence is shorter, it may be padded; if longer, it may be truncated.\n",
    "\n",
    "d_model = 8: Defines the size of each token’s embedding vector, meaning each token will be represented as a     8-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39030707-f514-4326-9f10-c8a9f2454aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 8\n",
    "max_sequence_length = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f85bce-72dd-43c4-9efa-9e122ab569c6",
   "metadata": {},
   "source": [
    "# Define three linear layers using nn.Linear in PyTorch:\n",
    "\n",
    "w_query: Projects input embeddings into query space.\n",
    "\n",
    "w_key: Projects input embeddings into key space.\n",
    "\n",
    "w_value: Projects input embeddings into value space.\n",
    "## These linear layers transform input embeddings (d_model dimensional) into new representations of the same size (d_model → d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ce832d-90e0-4fec-9ab5-b644a3673976",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_query = nn.Linear(d_model, d_model)\n",
    "w_key   = nn.Linear(d_model, d_model)\n",
    "w_value = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6556b4-f01c-4a2f-9409-f97cfdc2884d",
   "metadata": {},
   "source": [
    "# Create a tensor tokens with random values, to simulate a batch of sequence of token embeddings.\n",
    "\n",
    "Use torch.randn() to generate a random tensor of shape (batch_size, max_sequence_length, d_model), where:\n",
    "\n",
    "batch_size defines the number of sequences processed simultaneously\n",
    "\n",
    "max_sequence_length represents the number of tokens in the sequence.\n",
    "\n",
    "d_model represents the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e5d78ba-b6a4-4245-8caf-ffcabcd32a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "X_input_tokens = torch.randn( (batch_size, max_sequence_length, d_model) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb185a63-2804-4c76-ac90-97f8da038197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_input_tokens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2ef28ef-5897-4618-be30-06a43449774f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5722, -1.5508, -0.9508, -0.8640, -0.2197, -0.5222,  0.5066,\n",
       "           0.3882],\n",
       "         [-0.3872, -2.0749,  1.8568, -0.9443,  1.5268,  2.4347, -0.9094,\n",
       "          -0.4100],\n",
       "         [-0.1295,  0.8775, -1.2089, -0.8320,  0.0255, -1.4403, -0.2634,\n",
       "          -0.4547],\n",
       "         [-0.2725,  0.3093,  1.6942, -0.7240,  0.3211,  1.2152, -0.2805,\n",
       "          -0.8066],\n",
       "         [ 0.4247, -2.2672,  0.3589,  0.9826, -0.5440,  0.8139,  0.7595,\n",
       "           0.6310]],\n",
       "\n",
       "        [[-0.5642, -0.3357,  0.3224, -0.2222, -0.2079, -0.1331, -0.6866,\n",
       "           1.3085],\n",
       "         [-1.4051, -0.7623, -0.6487,  0.9436, -0.2093,  0.6138,  0.7549,\n",
       "          -0.2196],\n",
       "         [ 1.1316,  0.7940, -0.4966, -1.6428,  0.4345, -0.4395, -0.1694,\n",
       "           0.7543],\n",
       "         [ 0.4022,  0.2770, -0.7614,  0.8873, -1.5371,  0.0049, -0.1836,\n",
       "          -1.7946],\n",
       "         [-1.2220,  1.0616,  0.2727, -0.6134,  1.3609,  1.3732,  0.5116,\n",
       "           0.8456]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_input_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c611a1bf-1ce6-4d2e-af5c-df825bc27bb4",
   "metadata": {},
   "source": [
    "# Apply linear transformations to the tokens tensor using w_query, w_key, and w_value to obtain query (q), key (k), and value (v) representations.\n",
    "\n",
    "## Pass tokens through the three linear layers to compute q, k, and v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c86ac56-c293-4f10-a5eb-b2e7196c4f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = w_query(X_input_tokens)\n",
    "k = w_key(X_input_tokens)\n",
    "v = w_value(X_input_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531b88e5-baa8-427f-b486-fa144808cb66",
   "metadata": {},
   "source": [
    "# Compute Per-Head Dimensions in Multi-Head Attention\n",
    "In multi-head attention, we split the embedding dimension (d_model) into multiple heads to allow the model to focus on different parts of the input simultaneously.\n",
    "\n",
    "Define the number of attention heads as num_heads = 4.\n",
    "\n",
    "Compute the per-head query (d_q), key (d_k), and value (d_v) dimensions by dividing d_model by num_heads.\n",
    "\n",
    "Ensure that d_q, d_k, and d_v are equal and represent the dimension per head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28805687-d9f7-4709-91a4-20c6eff2feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 4\n",
    "d_q = d_model // num_heads\n",
    "d_k = d_model // num_heads\n",
    "d_v = d_model // num_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a559fc97-533e-4105-b5b6-0f51cd0c5057",
   "metadata": {},
   "source": [
    "# Reshape each tensor to (batch_size, max_sequence_length, num_heads, d_q), where:\n",
    "\n",
    "batch_size is the number of input sequences in a batch.\n",
    "\n",
    "max_sequence_length is the number of tokens per sequence.\n",
    "\n",
    "num_heads is the number of attention heads.\n",
    "\n",
    "d_q, d_k, and d_v are the per-head dimensions (d_model // num_heads).\n",
    "\n",
    "Verify that the new shape correctly divides the d_model dimension across multiple heads.\n",
    "\n",
    "Print the new shapes of q, k, and v to confirm the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "799c639e-2faf-4c49-967b-dfdf779b042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = q.reshape(batch_size, max_sequence_length, num_heads, d_q)\n",
    "k = k.reshape(batch_size, max_sequence_length, num_heads, d_k)\n",
    "v = v.reshape(batch_size, max_sequence_length, num_heads, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c98e4b9f-9f69-443a-a761-85287daaafcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 4, 2]), torch.Size([2, 5, 4, 2]), torch.Size([2, 5, 4, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a29ee3-4a40-40eb-860d-ea9ecfe784d0",
   "metadata": {},
   "source": [
    "# Transpose Query Tensor for Multi-Head Attention\n",
    "In multi-head attention, after reshaping the query (q), key (k), and value (v) tensors, we need to transpose them to bring the num_heads dimension to the second position. This helps in efficiently computing attention scores across multiple heads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b6a79-5420-448a-a40f-317394deb9f1",
   "metadata": {},
   "source": [
    "Assume you have a query tensor (q) of shape (batch_size, max_sequence_length, num_heads, d_q) after reshaping.\n",
    "\n",
    "Transpose q to rearrange dimensions, so that num_heads moves to the second position, resulting in (batch_size, num_heads, max_sequence_length, d_q).\n",
    "\n",
    "Print the shape of q after transposing to verify the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c2f7636-0548-47cb-93ea-21b8a258e82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 5, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = q.transpose(1, 2) # [batch_size, num_heads, sequence_length, d_q]\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb618295-c8df-4d89-82d3-efdd6bd84b7d",
   "metadata": {},
   "source": [
    "# Repeat the same operation for k and v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16876d4c-f466-44b8-b12d-b71fc05540f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 5, 2]), torch.Size([2, 4, 5, 2]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = k.transpose(1, 2) # [batch_size, num_heads, sequence_length, d_k]\n",
    "v = v.transpose(1, 2) # [batch_size, num_heads, sequence_length, d_v]\n",
    "k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994cfa0-e830-47b1-bec8-1616f38cc340",
   "metadata": {},
   "source": [
    "For a single head:\n",
    "$$\n",
    "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{new V} = \\text{self attention}.V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d41c0-624e-49f6-b419-a7c211748485",
   "metadata": {},
   "source": [
    "# Compute Attention Scores for Each Head\n",
    "In multi-head attention, after transposing q and k, we compute attention scores using scaled dot-product attention. This involves:\n",
    "\n",
    "Taking the dot product of q with the transposed k to get similarity scores.\n",
    "\n",
    "Scaling the scores by dividing by the square root of d_k (to stabilize gradients).\n",
    "\n",
    "Printing the shape of attn_scores to verify it per head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "540fd242-ee13-4ceb-9877-643f019bb0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 5, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float))\n",
    "attn_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4ff6ec9-dbb1-48ea-82f5-fc7b3af68282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 2, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.transpose(-1, -2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb853a84-f22f-41ea-af7b-d62118a3e205",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "- This is to ensure words don't get context from words generated in the future.\n",
    "- Not required in the encoders, but required in the decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e343aa37-d26b-4a82-9b4f-f80429972f13",
   "metadata": {},
   "source": [
    "# Create a lower triangular mask using torch.tril, which generates a matrix where only the lower triangle (including the diagonal) contains ones, while the upper triangle contains zeros. This mask is typically used in masked self-attention in transformers to ensure that each position in a sequence can only attend to previous positions and itself, preventing access to future tokens during decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34c219ed-b852-4d9d-87ec-a6a9f0d62abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones((max_sequence_length, max_sequence_length)))\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "254ed455-7ebe-4570-b305-3d84dda869fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d533cbf-b289-4c3b-a8c3-65a2298c3118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2502,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.4179, -0.1734,    -inf,    -inf,    -inf],\n",
       "          [ 0.0281,  0.1137, -0.0939,    -inf,    -inf],\n",
       "          [-0.6354, -0.5394, -0.1098, -0.2694,    -inf],\n",
       "          [-0.1238, -0.2060,  0.0896,  0.0569, -0.3147]],\n",
       "\n",
       "         [[-0.4693,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-1.0784, -0.4054,    -inf,    -inf,    -inf],\n",
       "          [ 0.2629, -0.2601, -0.2105,    -inf,    -inf],\n",
       "          [-0.4109, -0.2085,  0.0790, -0.0139,    -inf],\n",
       "          [-0.9293, -1.2110, -0.1219, -0.1696, -0.7876]],\n",
       "\n",
       "         [[-0.1552,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.1664, -0.4841,    -inf,    -inf,    -inf],\n",
       "          [ 0.0737,  0.2080,  0.2011,    -inf,    -inf],\n",
       "          [-0.0485, -0.1298, -0.1409, -0.0246,    -inf],\n",
       "          [-0.3010, -1.1245, -0.4832, -0.1315,  0.8267]],\n",
       "\n",
       "         [[-0.0373,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.1096, -0.0305,    -inf,    -inf,    -inf],\n",
       "          [-0.0115, -0.0059,  0.0126,    -inf,    -inf],\n",
       "          [-0.0797, -0.0312,  0.0825, -0.7984,    -inf],\n",
       "          [-0.0775,  0.0240,  0.0547,  0.0241,  0.0071]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1024,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.1579, -0.1965,    -inf,    -inf,    -inf],\n",
       "          [ 0.2351,  0.5439, -0.3724,    -inf,    -inf],\n",
       "          [-0.2479, -0.2655,  0.1877, -0.1413,    -inf],\n",
       "          [ 0.1178,  0.5753, -0.3881, -0.1256,  0.3337]],\n",
       "\n",
       "         [[-0.4666,    -inf,    -inf,    -inf,    -inf],\n",
       "          [-0.5380, -0.0278,    -inf,    -inf,    -inf],\n",
       "          [ 0.0892,  0.1052, -0.0137,    -inf,    -inf],\n",
       "          [ 0.2704,  0.2830, -0.0191,  0.4200,    -inf],\n",
       "          [-0.5895, -0.2649, -0.1768, -0.0749, -0.1400]],\n",
       "\n",
       "         [[-0.0610,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.0329,  0.3455,    -inf,    -inf,    -inf],\n",
       "          [-0.0897, -0.4112,  0.1900,    -inf,    -inf],\n",
       "          [-0.1262, -0.4175,  0.1718, -0.2955,    -inf],\n",
       "          [ 0.0416,  0.2084, -0.0987,  0.1805,  0.0335]],\n",
       "\n",
       "         [[ 0.0846,    -inf,    -inf,    -inf,    -inf],\n",
       "          [ 0.0884,  0.0103,    -inf,    -inf,    -inf],\n",
       "          [ 0.0804,  0.3445, -0.1617,    -inf,    -inf],\n",
       "          [ 0.2923,  0.3167, -0.1228,  0.1607,    -inf],\n",
       "          [ 0.0453,  0.3028, -0.1451, -0.5373,  0.2226]]]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7220fc17-db20-47f7-841a-b088864811ea",
   "metadata": {},
   "source": [
    "# Compute the weighted sum of value (v) vectors using attention weights, where each query token receives a context-aware representation. This ensures that each generated token attends to relevant past tokens, influencing its prediction based on learned dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df49fc69-9964-4211-818e-11515ff41ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = F.softmax(attn_scores, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4109ddf-f398-45ce-a764-0d36bb3fd72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4392, 0.5608, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3362, 0.3662, 0.2976, 0.0000, 0.0000],\n",
       "          [0.1911, 0.2103, 0.3232, 0.2755, 0.0000],\n",
       "          [0.1929, 0.1777, 0.2388, 0.2311, 0.1594]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3378, 0.6622, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4513, 0.2675, 0.2811, 0.0000, 0.0000],\n",
       "          [0.1871, 0.2291, 0.3054, 0.2783, 0.0000],\n",
       "          [0.1372, 0.1035, 0.3077, 0.2934, 0.1581]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5788, 0.4212, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3049, 0.3487, 0.3464, 0.0000, 0.0000],\n",
       "          [0.2592, 0.2390, 0.2363, 0.2655, 0.0000],\n",
       "          [0.1528, 0.0671, 0.1273, 0.1810, 0.4719]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4802, 0.5198, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3300, 0.3319, 0.3381, 0.0000, 0.0000],\n",
       "          [0.2693, 0.2827, 0.3167, 0.1313, 0.0000],\n",
       "          [0.1837, 0.2033, 0.2097, 0.2034, 0.1999]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5096, 0.4904, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3441, 0.4685, 0.1874, 0.0000, 0.0000],\n",
       "          [0.2155, 0.2117, 0.3331, 0.2397, 0.0000],\n",
       "          [0.1920, 0.3034, 0.1158, 0.1505, 0.2383]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3751, 0.6249, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3427, 0.3482, 0.3092, 0.0000, 0.0000],\n",
       "          [0.2549, 0.2582, 0.1908, 0.2961, 0.0000],\n",
       "          [0.1402, 0.1939, 0.2118, 0.2345, 0.2197]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4225, 0.5775, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3281, 0.2379, 0.4340, 0.0000, 0.0000],\n",
       "          [0.2539, 0.1897, 0.3420, 0.2143, 0.0000],\n",
       "          [0.1926, 0.2276, 0.1674, 0.2213, 0.1911]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5195, 0.4805, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3239, 0.4218, 0.2543, 0.0000, 0.0000],\n",
       "          [0.2808, 0.2877, 0.1854, 0.2462, 0.0000],\n",
       "          [0.2052, 0.2655, 0.1696, 0.1146, 0.2450]]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3959bb1-c655-4c3d-b43e-9bf114694d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output = torch.matmul(attn_weights, v).transpose(1, 2).contiguous().view(batch_size, max_sequence_length, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac7c051c-b61f-4ec2-a48b-ceba9e023562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 8])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bed78eb-c8b4-4c02-923d-16772b0982a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[     0.5889,     -0.2544,      0.5796,     -0.2053,      0.8867,\n",
       "               0.8584,     -0.1849,     -0.0656],\n",
       "         [     1.3559,      0.1895,      0.1792,      0.5596,      0.5102,\n",
       "               0.4807,      0.2403,     -0.2482],\n",
       "         [     0.8766,     -0.1489,      0.1727,      0.2288,      0.4115,\n",
       "               0.1955,      0.1986,     -0.1561],\n",
       "         [     0.7896,     -0.1617,     -0.0589,      0.5489,      0.2659,\n",
       "               0.1213,      0.3184,     -0.1361],\n",
       "         [     0.7703,     -0.0865,     -0.0439,      0.3919,      0.2174,\n",
       "               0.4571,      0.2243,     -0.2422]],\n",
       "\n",
       "        [[    -0.1676,      0.1436,     -0.1614,      0.3819,     -0.1275,\n",
       "              -0.3200,     -0.2202,     -0.5121],\n",
       "         [    -0.0581,     -0.3306,     -0.4269,      0.1313,      0.0746,\n",
       "              -0.0504,     -0.2902,     -0.6253],\n",
       "         [    -0.0008,     -0.2957,     -0.2886,      0.4192,      0.3742,\n",
       "              -0.0817,     -0.0170,     -0.3264],\n",
       "         [    -0.0525,     -0.2981,     -0.0815,      0.0867,      0.2448,\n",
       "               0.1383,     -0.0859,     -0.3750],\n",
       "         [     0.0290,     -0.2709,     -0.3491,      0.4852,      0.2478,\n",
       "               0.0286,      0.2555,     -0.1676]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f2dfc1-ca21-4a7d-bd5d-528cb7b77ed0",
   "metadata": {},
   "source": [
    "# Task-1: Implement MultiHeadAttention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "caaa5ab9-7d5e-4c50-ae3f-a67280be7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.d_v = d_model // num_heads\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q_input, k_input, v_input, mask=None):\n",
    "\n",
    "        batch_size, max_sequence_length, _ = q_input.size()\n",
    "        \n",
    "        Q = self.query(q_input)\n",
    "        K = self.key(k_input)\n",
    "        V = self.value(v_input)\n",
    "        \n",
    "        q = Q.reshape(batch_size, max_sequence_length, self.num_heads, self.d_k)\n",
    "        k = K.reshape(batch_size, max_sequence_length, self.num_heads, self.d_k)\n",
    "        v = V.reshape(batch_size, max_sequence_length, self.num_heads, self.d_v)\n",
    "        \n",
    "        q = q.transpose(1, 2) # [batch_size, num_heads, max_sequence_length, d_k]\n",
    "        k = k.transpose(1, 2) # [batch_size, num_heads, max_sequence_length, d_k]\n",
    "        v = v.transpose(1, 2) # [batch_size, num_heads, max_sequence_length, d_v]\n",
    "\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float))\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))  # Ensure mask matches input length\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "    \n",
    "        attention_output = torch.matmul(attn_weights, v).transpose(1, 2).contiguous().view(batch_size, max_sequence_length, self.d_model)\n",
    "       \n",
    "        output = self.fc(attention_output)\n",
    "        return output\n",
    "    \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "997cb08d-a9b8-40fa-927f-569c87e5df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 16  # Small model for testing\n",
    "num_heads = 4  # Number of heads\n",
    "seq_len = 5  # Sequence length\n",
    "batch_size = 2  # Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e35e4bfb-2538-4571-a172-ef22b177524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random input tensor\n",
    "x = torch.rand((batch_size, seq_len, d_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a72df-711f-43f1-87aa-a2ecb5254f47",
   "metadata": {},
   "source": [
    "# Self-Attention in the Encoder\n",
    "## Purpose: Allows each token to attend to all other tokens in the input sequence.\n",
    "\n",
    "### Masking: No causal masking (tokens can see all positions).\n",
    "\n",
    "#### Input: x (same for query, key, value).\n",
    "\n",
    "#### Mask: Usually a padding mask (not needed for random input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f1c15f2-0127-44c2-b0c3-2af874b2bc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 16])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_enc = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "mask = None\n",
    "\n",
    "# Forward pass\n",
    "output = mha_enc(x, x, x, mask)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195de41-cb00-4667-94a7-1b0e3cc1f856",
   "metadata": {},
   "source": [
    "# Masked Self-Attention in the Decoder\n",
    "## Purpose: Prevents each token from attending to future tokens.\n",
    "\n",
    "### Masking: Causal mask applied.\n",
    "\n",
    "### Input: x (same for query, key, value).\n",
    "\n",
    "### Mask: Lower triangular mask to enforce causality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a29a5305-260c-4479-90ce-5bbfdb66b706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 16])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_dec = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "mask = torch.tril(torch.ones((max_sequence_length, max_sequence_length)))\n",
    "\n",
    "# Forward pass\n",
    "output = mha_dec(x, x, x, mask)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b4b86-3a13-4ce4-acd6-8bf65eeb48a5",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Cross-Attention in the Decoder\n",
    "## Purpose: Allows decoder tokens to attend to all encoder tokens.\n",
    "\n",
    "## Masking: No causal mask (attends to all encoder tokens).\n",
    "\n",
    "### Input:\n",
    "\n",
    "#### q (decoder representation).\n",
    "\n",
    "#### k, v (encoder output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77b20962-8a5c-4d12-ab45-005984ed9d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Initialize Multi-Head Attention\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Create random input tensors\n",
    "decoder_input = torch.rand((batch_size, seq_len, d_model))  # Query from decoder\n",
    "encoder_output = torch.rand((batch_size, seq_len, d_model))  # Key & Value from encoder\n",
    "\n",
    "# No causal mask needed for encoder-decoder cross-attention\n",
    "mask = None\n",
    "\n",
    "# Forward pass\n",
    "cross_attn_output = mha(decoder_input, encoder_output, encoder_output, mask)\n",
    "print(cross_attn_output.shape)  # Expected: (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af6209-4a3e-4ba7-8154-7fe4b68cbacd",
   "metadata": {},
   "source": [
    "# Understanding Contiguous and Non-Contiguous Tensors in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0614019a-8779-4ce7-83d2-c9852e0c6c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      " tensor([[ 0.4496, -0.3525,  0.7069],\n",
      "        [ 2.0206,  0.1058,  0.8492]])\n",
      "Is contiguous? True\n",
      "\n",
      "Transposed tensor:\n",
      " tensor([[ 0.4496,  2.0206],\n",
      "        [-0.3525,  0.1058],\n",
      "        [ 0.7069,  0.8492]])\n",
      "Is contiguous? False\n"
     ]
    }
   ],
   "source": [
    "# Create a contiguous tensor\n",
    "x = torch.randn(2, 3)\n",
    "print(\"Original tensor:\\n\", x)\n",
    "print(\"Is contiguous?\", x.is_contiguous())  # True\n",
    "\n",
    "# Transpose it (creates a non-contiguous tensor)\n",
    "x_t = x.transpose(0, 1)\n",
    "print(\"\\nTransposed tensor:\\n\", x_t)\n",
    "print(\"Is contiguous?\", x_t.is_contiguous())  # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4214c9-c3f8-41be-aaa4-4eb60fdb075b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
