{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c29bb37-abe5-4b9f-9c5f-e9db8328c37f",
   "metadata": {},
   "source": [
    "# Demonstrating Positional Encoding in Transformers using PyTorch\n",
    "## In this notebook, we will explore positional encoding, a crucial component of Transformer models that allows them to capture the order of tokens in a sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb4bfd6-c593-4b7a-9a40-6b82e327555b",
   "metadata": {},
   "source": [
    "### **Positional Encoding Formula**\n",
    "In Transformers, **positional encoding** provides information about the order of tokens in a sequence. The formula for computing positional encodings is:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\bigg(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\bigg(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\bigg)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( pos \\) is the **position** of a token in the sequence.\n",
    "- \\( i \\) is the **dimension index** in the embedding vector.\n",
    "- \\( d_{model} \\) is the **embedding dimension**.\n",
    "- The denominator scales the positions to different frequencies.\n",
    "\n",
    "These encodings are **added** to input embeddings before passing them into the Transformer model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb7a58a-2b0a-49b6-91b9-dcc21aa59a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977f96a6-f919-4df8-9b7f-e60c9e55458f",
   "metadata": {},
   "source": [
    "# Define the maximum sequence length and the embedding dimension for a model:\n",
    "\n",
    "max_sequence_length = 10: Specifies the maximum number of tokens a sequence can have. If a sequence is shorter, it may be padded; if longer, it may be truncated.\n",
    "\n",
    "d_model = 6: Defines the size of each tokenâ€™s embedding vector, meaning each token will be represented as a 6-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1261755-0a6f-4e1b-bfc1-14e6fa312153",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 10\n",
    "d_model = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee716d92-2d70-41cc-8c95-406a96208a0e",
   "metadata": {},
   "source": [
    "# Initializes and returns a sequence of floating-point values for half of the embedding dimension:\n",
    "\n",
    "Creates a tensor with values ranging from 0 to (d_model//2 - 1).\n",
    "\n",
    "Converts it to a floating-point tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3203e6-e60e-4ae9-b8b3-edc0c35ba858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.arange(0, d_model//2).float()\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7822e-ff6f-4532-8188-cf9819af0469",
   "metadata": {},
   "source": [
    "# Compute a denominator tensor used in positional encoding for Transformers:\n",
    "\n",
    "torch.pow(10000, 2*i/d_model):\n",
    "\n",
    "Raises 10000 to the power of (2*i / d_model), where i is a tensor of indices.\n",
    "\n",
    "Helps in generating sinusoidal positional encodings with different wavelengths for different embedding dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a0ee1aa-5953-45f4-81d5-65c87ceea91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.0000,  21.5443, 464.1590])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denominator = torch.pow(10000, 2*i/d_model)\n",
    "denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ea128-8afd-4e4b-b244-148ea994c558",
   "metadata": {},
   "source": [
    "# Create a tensor representing token positions in a sequence:\n",
    "\n",
    "Generates a sequence of numbers from 0 to max_sequence_length - 1, represents the position of each token in the sequence.\n",
    "\n",
    "Reshape the tensor into a column vector of shape (max_sequence_length, 1), making it compatible for further computations like positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f73d8bd-2519-4da0-84fe-0e3fd3f48007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.],\n",
       "        [9.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_pos = torch.arange(max_sequence_length, dtype=torch.float).reshape(max_sequence_length, 1)\n",
    "token_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7356215a-a931-45be-840a-40a43f783370",
   "metadata": {},
   "source": [
    "# Initialize a positional encoding matrix with zeros:\n",
    "\n",
    "Create a tensor of shape (max_sequence_length, d_model), filled with zeros.\n",
    "\n",
    "Acts as a placeholder for storing positional encodings for each token position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d0371d7-526c-4f1c-b7df-1c1749087dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE = torch.zeros(max_sequence_length, d_model)\n",
    "PE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b85bd-6dde-40ff-9a9b-26c7198ee2b9",
   "metadata": {},
   "source": [
    "# Update the positional encoding matrix with sine values for even indices:\n",
    "\n",
    "Compute the sine function of token_pos / denominator.\n",
    "\n",
    "Assign the result to even-indexed columns (0, 2, 4, ...) of PE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d1a5811-e71a-4ad8-8ec2-93ece1f41f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.8415,  0.0000,  0.0464,  0.0000,  0.0022,  0.0000],\n",
       "        [ 0.9093,  0.0000,  0.0927,  0.0000,  0.0043,  0.0000],\n",
       "        [ 0.1411,  0.0000,  0.1388,  0.0000,  0.0065,  0.0000],\n",
       "        [-0.7568,  0.0000,  0.1846,  0.0000,  0.0086,  0.0000],\n",
       "        [-0.9589,  0.0000,  0.2300,  0.0000,  0.0108,  0.0000],\n",
       "        [-0.2794,  0.0000,  0.2749,  0.0000,  0.0129,  0.0000],\n",
       "        [ 0.6570,  0.0000,  0.3192,  0.0000,  0.0151,  0.0000],\n",
       "        [ 0.9894,  0.0000,  0.3629,  0.0000,  0.0172,  0.0000],\n",
       "        [ 0.4121,  0.0000,  0.4057,  0.0000,  0.0194,  0.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE[:, 0::2] = torch.sin(token_pos / denominator)\n",
    "PE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f74c16f-2076-4163-af23-11f0315d12d1",
   "metadata": {},
   "source": [
    "# Update the positional encoding matrix with cosine values for odd indices:\n",
    "\n",
    "Compute the cosine function of token_pos / denominator.\n",
    "\n",
    "Assign the result to odd-indexed columns (1, 3, 5, ...) of PE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a1874ae-936e-4cbc-aa06-769f5ce309f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
       "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
       "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
       "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
       "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
       "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
       "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
       "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
       "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE[:, 1::2] = torch.cos(token_pos / denominator)\n",
    "PE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38db0e-af6a-485a-a63d-e04486859bdb",
   "metadata": {},
   "source": [
    "# Implement a PositionalEncoding Class\n",
    "## Write a PyTorch class PositionalEncoding, which:\n",
    "### 1) Accepts parameters for embedding dimension (d_model) and maximum sequence length (max_len).\n",
    "### 2) Defines a forward() method that\n",
    "#### 2.1) Computes sinusoidal positional embeddings and stores them as a tensor.\n",
    "#### 2.2) Returns the positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b5fc1bc-2a19-47a2-8cb3-8b72be721bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "        self.PE = torch.zeros(max_sequence_length, d_model)\n",
    "\n",
    "    def forward(self):\n",
    "        i = torch.arange(0, self.d_model//2).float()\n",
    "        denominator = torch.pow(10000, 2*i/self.d_model)\n",
    "        token_pos = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
    "        self.PE[:, 0::2] = torch.sin(token_pos / denominator)\n",
    "        self.PE[:, 1::2] = torch.cos(token_pos / denominator)\n",
    "        return PE  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655bd99b-489f-4ba4-9863-c300b2f82321",
   "metadata": {},
   "source": [
    "# Initializes a positional encoding with d_model=6 and max_sequence_length=10, then computes the encodings via forward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6354539-b616-4486-9347-f7ebbd5f51bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
       "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
       "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
       "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
       "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
       "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
       "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
       "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
       "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionalEncoding(d_model=6, max_sequence_length=10)\n",
    "pe.forward() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
