{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "588dbdcc-264b-4c62-911f-55b706e08df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79320154-1f2e-42de-a26f-bcd3d1dcdcb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd37bc9b8b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(24)  # Python random seed\n",
    "np.random.seed(24)  # NumPy seed\n",
    "torch.manual_seed(24)  # PyTorch seed (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2745f2-4ca8-440f-8ea1-19f7637111f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set print options: No scientific notation, 2 decimal places\n",
    "torch.set_printoptions(sci_mode=False, precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f2dfc1-ca21-4a7d-bd5d-528cb7b77ed0",
   "metadata": {},
   "source": [
    "# Task-1: Implement MultiHeadAttention class"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1703b176-3d95-42e5-a9ad-a079264a5856",
   "metadata": {},
   "source": [
    "Your task is to implement the MultiHeadAttention class, a key component of the Transformer model. This module enables the model to focus on different parts of the input sequence simultaneously using multiple attention heads.\n",
    "\n",
    "You will implement the following functionalities:\n",
    "1.Initialization:\n",
    "\n",
    "    Define the parameters d_model (input and output dimensionality) and num_heads (number of attention heads).\n",
    "    Compute d_k and d_v, the dimensionality of each attention head.\n",
    "    Define linear transformations for computing query (self.query), key (self.key), and value (self.value).\n",
    "    Define the final fully connected layer (self.fc) to project the concatenated outputs back to d_model.\n",
    "2.Forward Pass:\n",
    "\n",
    "    Compute query (q), key (k), and value (v) vectors by applying the respective linear transformations.\n",
    "    Reshape q, k, and v to split them across multiple heads, ensuring each head gets d_k and d_v dimensions.\n",
    "\n",
    "    Compute scaled dot-product attention using the formula:\n",
    "\n",
    "     Apply a mask if provided (mask), ensuring masked positions do not influence attention.\n",
    "\n",
    "    Compute attention weights (attn_weights) using the Softmax function.\n",
    "\n",
    "    Apply the attention weights to the value (v) tensor, producing attention_output.\n",
    "\n",
    "    Reshape and concatenate all attention heads to get back to the original d_model dimension.\n",
    "\n",
    "    Pass the result through the final fully connected layer (self.fc) to obtain the final attention output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caaa5ab9-7d5e-4c50-ae3f-a67280be7980",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d14401d79c50c545",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "    ### BEGIN SOLUTION\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.d_v = d_model // num_heads\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    def forward(self, q_input, k_input, v_input, mask=None):\n",
    "    ### BEGIN SOLUTION\n",
    "        batch_size, max_sequence_length, _ = q_input.size()\n",
    "        \n",
    "        Q = self.query(q_input)\n",
    "        K = self.key(k_input)\n",
    "        V = self.value(v_input)\n",
    "        \n",
    "        q = Q.reshape(batch_size, max_sequence_length, self.num_heads, self.d_k)\n",
    "        k = K.reshape(batch_size, max_sequence_length, self.num_heads, self.d_k)\n",
    "        v = V.reshape(batch_size, max_sequence_length, self.num_heads, self.d_v)\n",
    "        \n",
    "        q = q.transpose(1, 2) # [batch_size, num_heads, max_sequence_length, d_k]\n",
    "        k = k.transpose(1, 2) # [batch_size, num_heads, max_sequence_length, d_k]\n",
    "        v = v.transpose(1, 2) # [batch_size, num_heads, max_sequence_length, d_v]\n",
    "\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float))\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))  # Ensure mask matches input length\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "    \n",
    "        attention_output = torch.matmul(attn_weights, v).transpose(1, 2).contiguous().view(batch_size, max_sequence_length, self.d_model)\n",
    "       \n",
    "        output = self.fc(attention_output)\n",
    "        return output\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96c53dc0-1145-4716-9330-8dbdc68e8093",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-dbaa77482dc8a7b7",
     "locked": true,
     "points": 100,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Define a simple test function\n",
    "def test_multi_head_attention():\n",
    "    d_model_t = 8  # Small for testing\n",
    "    num_heads_t = 2\n",
    "    batch_size_t = 4\n",
    "    max_seq_length_t = 5\n",
    "\n",
    "    mha_t = MultiHeadAttention(d_model_t, num_heads_t)\n",
    "\n",
    "    q_input_t = torch.randn(batch_size_t, max_seq_length_t, d_model_t)\n",
    "    k_input_t = torch.randn(batch_size_t, max_seq_length_t, d_model_t)\n",
    "    v_input_t = torch.randn(batch_size_t, max_seq_length_t, d_model_t)\n",
    "\n",
    "    # Forward pass\n",
    "    output_t = mha_t(q_input_t, k_input_t, v_input_t)\n",
    "\n",
    "    # Check output shape\n",
    "    assert output_t.shape == (batch_size_t, max_seq_length_t, d_model_t), \\\n",
    "        f\"Expected shape {(batch_size_t, max_seq_length_t, d_model_t)}, got {output_t.shape}\"\n",
    "\n",
    "    # Check if output is differentiable (should require gradients)\n",
    "    assert output_t.requires_grad, \"Output should require gradients\"\n",
    "\n",
    "    # Ensure attention weights sum to 1\n",
    "    with torch.no_grad():\n",
    "        q_proj_t = mha_t.query(q_input_t).reshape(batch_size_t, max_seq_length_t, num_heads_t, mha_t.d_k).transpose(1, 2)\n",
    "        k_proj_t = mha_t.key(k_input_t).reshape(batch_size_t, max_seq_length_t, num_heads_t, mha_t.d_k).transpose(1, 2)\n",
    "\n",
    "        attn_scores_t = torch.matmul(q_proj_t, k_proj_t.transpose(-2, -1)) / torch.sqrt(torch.tensor(mha_t.d_k, dtype=torch.float))\n",
    "        attn_weights_t = F.softmax(attn_scores_t, dim=-1)\n",
    "\n",
    "        assert torch.allclose(attn_weights_t.sum(dim=-1), torch.ones_like(attn_weights_t.sum(dim=-1))), \\\n",
    "            \"Attention weights should sum to 1 across the sequence dimension\"\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "# Run the test function\n",
    "test_multi_head_attention()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "997cb08d-a9b8-40fa-927f-569c87e5df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 16  # Small model for testing\n",
    "num_heads = 4  # Number of heads\n",
    "max_sequence_length = 5  # Sequence length\n",
    "batch_size = 2  # Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e35e4bfb-2538-4571-a172-ef22b177524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random input tensor\n",
    "x = torch.rand((batch_size, max_sequence_length, d_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a72df-711f-43f1-87aa-a2ecb5254f47",
   "metadata": {},
   "source": [
    "# Self-Attention in the Encoder\n",
    "## Purpose: Allows each token to attend to all other tokens in the input sequence.\n",
    "\n",
    "### Masking: No causal masking (tokens can see all positions).\n",
    "\n",
    "#### Input: x (same for query, key, value).\n",
    "\n",
    "#### Mask: Usually a padding mask (not needed for random input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f1c15f2-0127-44c2-b0c3-2af874b2bc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_enc = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "mask = None\n",
    "\n",
    "# Forward pass\n",
    "output = mha_enc(x, x, x, mask)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195de41-cb00-4667-94a7-1b0e3cc1f856",
   "metadata": {},
   "source": [
    "# Masked Self-Attention in the Decoder\n",
    "## Purpose: Prevents each token from attending to future tokens.\n",
    "\n",
    "### Masking: Causal mask applied.\n",
    "\n",
    "### Input: x (same for query, key, value).\n",
    "\n",
    "### Mask: Lower triangular mask to enforce causality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a29a5305-260c-4479-90ce-5bbfdb66b706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_dec = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "mask = torch.tril(torch.ones((max_sequence_length, max_sequence_length)))\n",
    "\n",
    "# Forward pass\n",
    "output = mha_dec(x, x, x, mask)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b4b86-3a13-4ce4-acd6-8bf65eeb48a5",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Cross-Attention in the Decoder\n",
    "## Purpose: Allows decoder tokens to attend to all encoder tokens.\n",
    "\n",
    "## Masking: No causal mask (attends to all encoder tokens).\n",
    "\n",
    "### Input:\n",
    "\n",
    "#### q (decoder representation).\n",
    "\n",
    "#### k, v (encoder output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77b20962-8a5c-4d12-ab45-005984ed9d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Initialize Multi-Head Attention\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Create random input tensors\n",
    "decoder_input = torch.rand((batch_size, max_sequence_length, d_model))  # Query from decoder\n",
    "encoder_output = torch.rand((batch_size, max_sequence_length, d_model))  # Key & Value from encoder\n",
    "\n",
    "# No causal mask needed for encoder-decoder cross-attention\n",
    "mask = None\n",
    "\n",
    "# Forward pass\n",
    "cross_attn_output = mha(decoder_input, encoder_output, encoder_output, mask)\n",
    "print(cross_attn_output.shape)  # Expected: (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af6209-4a3e-4ba7-8154-7fe4b68cbacd",
   "metadata": {},
   "source": [
    "# Understanding Contiguous and Non-Contiguous Tensors in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0614019a-8779-4ce7-83d2-c9852e0c6c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      " tensor([[ 0.8104,  0.0467,  1.0917],\n",
      "        [-0.5887,  0.4677,  1.1150]])\n",
      "Is contiguous? True\n",
      "\n",
      "Transposed tensor:\n",
      " tensor([[ 0.8104, -0.5887],\n",
      "        [ 0.0467,  0.4677],\n",
      "        [ 1.0917,  1.1150]])\n",
      "Is contiguous? False\n"
     ]
    }
   ],
   "source": [
    "# Create a contiguous tensor\n",
    "x = torch.randn(2, 3)\n",
    "print(\"Original tensor:\\n\", x)\n",
    "print(\"Is contiguous?\", x.is_contiguous())  # True\n",
    "\n",
    "# Transpose it (creates a non-contiguous tensor)\n",
    "x_t = x.transpose(0, 1)\n",
    "print(\"\\nTransposed tensor:\\n\", x_t)\n",
    "print(\"Is contiguous?\", x_t.is_contiguous())  # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4214c9-c3f8-41be-aaa4-4eb60fdb075b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
