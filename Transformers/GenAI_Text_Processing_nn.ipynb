{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a51ed8-ab4a-469c-9e2c-41109de36fc3",
   "metadata": {},
   "source": [
    "# nn.Embedding in PyTorch\n",
    "## nn.Embedding is a lookup table that maps discrete indices (e.g., words, tokens, or categories) to continuous vector representations (embeddings). It is commonly used in NLP and recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c514e-8555-47f1-82f9-a472485bd1aa",
   "metadata": {},
   "source": [
    "### nn.Embedding(num_embeddings, embedding_dim, padding_idx=None,..)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1db404-f12c-43c1-a952-9c0c707c3381",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "num_embeddings: Total number of possible indices (e.g., vocabulary size).\n",
    "\n",
    "embedding_dim: Size of each embedding vector.\n",
    "\n",
    "padding_idx (optional): Specifies an index (e.g., 0) whose embedding remains zero during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72419184-0ee2-4477-ba50-7bd4e6458000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c82122b7-9b08-460f-8317-adaac49125c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example vocabulary\n",
    "vocab = {\"hello\": 0, \"world\": 1, \"this\": 2, \"is\": 3, \"a\": 4, \"test\": 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c701c10e-784b-4022-9a25-b32d093cb347",
   "metadata": {},
   "source": [
    "# Define an embedding layer with vocab size 6 and embedding dimension 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6232d585-f1b8-43df-8b96-53757bdd7540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an embedding layer with vocab size 6 and embedding dimension 3\n",
    "embedding_layer = nn.Embedding(num_embeddings=6, embedding_dim=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b8f40-78f9-43f8-967a-2ce83532394e",
   "metadata": {},
   "source": [
    "# Setence_1 = \"this is a test\"\n",
    "# Convert words to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6cef8e-8f8a-49a5-951f-17960e5bdd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence/Sequence\n",
    "# Setence_1 = \"this is a test\"\n",
    "# Convert words to indices\n",
    "Setence_1_indices = torch.tensor([2, 3, 4, 5])  # \"this\", \"is\", \"a\", \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa47a98-e5b5-476b-8dd1-87f8465157ec",
   "metadata": {},
   "source": [
    "# Get the embeddings for these words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb5c2fd-ded6-499d-bc2a-13b5358262de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2431, -1.0259, -1.0153],\n",
      "        [ 0.3792,  0.7361,  0.4511],\n",
      "        [-1.1624, -0.3854, -0.2508],\n",
      "        [-0.8085, -0.2324,  1.5288]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get the embeddings for these words\n",
    "embeddings = embedding_layer(Setence_1_indices)\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f726709d-df63-4d07-b524-6107227de215",
   "metadata": {},
   "source": [
    "# Repeat for Setence_2 = \"hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97a58937-463f-4661-b440-32f8499c8590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2121, -1.1920, -0.4230],\n",
      "        [ 0.0727,  0.8886,  1.2703]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Sentence ya Sequence\n",
    "# Setence_2 = \"hello world\"\n",
    "# Convert words to indices\n",
    "Setence_2_indices = torch.tensor([0, 1])  # \"hello\", \"world\"\n",
    "\n",
    "# Get the embeddings for these words\n",
    "embeddings = embedding_layer(Setence_2_indices)\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eaa137-647d-4ac3-8b8d-25c97df02fed",
   "metadata": {},
   "source": [
    "# Example vocabulary including a padding token (index 0)\n",
    "# The padding token will be represented by index 0 in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3668c73f-4ff0-415a-b1e7-42d2f6dabf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example vocabulary including a padding token (index 0)\n",
    "# The padding token will be represented by index 0 in the vocabulary.\n",
    "vocab = {\"hello\": 1, \"world\": 2, \"this\": 3, \"is\": 4, \"a\": 5, \"test\": 6, \"<pad>\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea2f6bb-1a5a-43f4-9d54-4497d196bcec",
   "metadata": {},
   "source": [
    "# Define an embedding layer with vocab size 7 (6 words + 1 padding token) and embedding dimension 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1144bcf-ef34-4f62-9887-e8345699dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an embedding layer with vocab size 7 (6 words + 1 padding token) and embedding dimension 3\n",
    "embedding_layer = nn.Embedding(num_embeddings=7, embedding_dim=3, padding_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e18c6-daa9-4864-8e89-681223e59f76",
   "metadata": {},
   "source": [
    "# Define two sequences of different lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "434143f7-3b5c-4ad0-b7d3-c6e3024536a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two sequences of different lengths\n",
    "sequence_1 = [3, 4, 5, 6]  # Corresponds to [\"this\", \"is\", \"a\", \"test\"]\n",
    "sequence_2 = [1, 2]        # Corresponds to [\"hello\", \"world\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e31fd0-177e-495d-b7f3-a6c5c240c7dc",
   "metadata": {},
   "source": [
    "# Define the fixed length for padding\n",
    "# Pad the sequences to the fixed length using the <pad> token (index 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8df83d20-858e-4282-afcf-b4e3119f1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fixed length for padding\n",
    "fixed_length = 5\n",
    "\n",
    "# Pad the sequences to the fixed length using the <pad> token (index 0)\n",
    "# The pad token is used to make both sequences of length `fixed_length`\n",
    "padded_sequence_1 = sequence_1 + [0] * (fixed_length - len(sequence_1))  # Pad sequence_1\n",
    "padded_sequence_2 = sequence_2 + [0] * (fixed_length - len(sequence_2))  # Pad sequence_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b36b9519-ad2c-4eb7-92fc-1250ad9f1dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [3, 4, 5, 6, 0]\n",
      " [1, 2, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\" {padded_sequence_1}\")\n",
    "print(f\" {padded_sequence_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8a5245-0f04-44cd-b95f-4dc63be12181",
   "metadata": {},
   "source": [
    "# Convert sequences to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff33b823-dc7f-4fc4-a19e-ef9c6ce8eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sequences to tensors\n",
    "indices_1 = torch.tensor(padded_sequence_1)  # Sequence 1 after padding\n",
    "indices_2 = torch.tensor(padded_sequence_2)  # Sequence 2 after padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16616201-32b4-42fd-ae63-4b83c644b58c",
   "metadata": {},
   "source": [
    "# Get the embeddings for these padded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ed644bd-e820-4431-bc36-f3c9e2eb557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embeddings for these padded sequences\n",
    "embeddings_1 = embedding_layer(indices_1)  # Embeddings for sequence 1\n",
    "embeddings_2 = embedding_layer(indices_2)  # Embeddings for sequence 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118687e0-9861-4109-9c30-361efdc3fdda",
   "metadata": {},
   "source": [
    "# Print the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8063b2fc-3778-4a90-ad5e-00b642e0182d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for sequence 1:\n",
      "tensor([[ 1.5508, -1.2233, -1.7642],\n",
      "        [ 0.3967, -0.3848,  0.1251],\n",
      "        [ 1.7091,  0.1736, -1.7899],\n",
      "        [ 0.1929, -0.0116, -0.7650],\n",
      "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Embeddings for sequence 2:\n",
      "tensor([[-0.8736,  0.6329,  1.5281],\n",
      "        [ 0.0240,  0.3783,  1.7073],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Print the embeddings\n",
    "print(\"Embeddings for sequence 1:\")\n",
    "print(embeddings_1)\n",
    "\n",
    "print(\"\\nEmbeddings for sequence 2:\")\n",
    "print(embeddings_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3a639c-4b3d-42a8-8b42-5ef6f532092c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
