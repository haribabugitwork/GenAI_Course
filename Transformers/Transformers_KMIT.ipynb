{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utdkf8xUaoCL"
   },
   "source": [
    "# Step 1: Tokenizer and Vocabulary Creation (Add more words if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIZEbVpAaoCM"
   },
   "source": [
    "## 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "9oTx376PaoCN"
   },
   "source": [
    "Import the required PyTorch modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UsjM65OiaoCN",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-93e1006ec1e1082c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "UxPKHl19aoCP"
   },
   "source": [
    "You are working on building a simple machine translation model. The first step in the pipeline is to tokenize the given sentences and create vocabulary mappings for both the source and target languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LlPgKXo1aoCQ",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dc6465894d399755",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Tokenizer and Vocabulary Creation (Add more words if needed)\n",
    "sentence_en = \"I love AI .\"\n",
    "# sentence_fr = \"J' adore l'IA .\"\n",
    "sentence_fr = \"నాకు ధోని అంటే ఇష్టం .\"\n",
    "\n",
    "word_map_en = {\"<pad>\": 0, \"I\": 1, \"love\": 2, \"AI\": 3, \".\": 4}\n",
    "word_map_fr = {\"<pad>\": 0, \"నాకు\": 1, \"ధోని\": 2, \"అంటే\": 3, \"ఇష్టం\": 4, \".\": 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjF0KpodaoCQ"
   },
   "source": [
    "## 2. Tokenizing sentences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "kNFS5hCgaoCR"
   },
   "source": [
    "Implement the tokenize function to map each word in the sentence to the corresponding index from the word_map.\n",
    "Ensure that the function returns a PyTorch tensor containing the word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8PqFacDtaoCR",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d7aeab8af7471314",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenizing sentences\n",
    "def tokenize(sentence, word_map):\n",
    "    return torch.tensor([word_map[word] for word in sentence.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vd3pKkKYaoCS"
   },
   "source": [
    "## 3.Tokenize the input and target sentences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "-qDggjfGaoCT"
   },
   "source": [
    "Tokenize the input (sentence_en) and target (sentence_fr) sentences.\n",
    "Add the extra dimension to each tokenized tensor using .unsqueeze(0) to make the shape (1, N)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WM51O1sLaoCT",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5df70d1e37703f35",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the input and target sentences\n",
    "input_tensor = tokenize(sentence_en, word_map_en).unsqueeze(0)  # Shape (1, 6)\n",
    "target_tensor = tokenize(sentence_fr, word_map_fr).unsqueeze(0)  # Shape (1, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZnQiUqdaoCT"
   },
   "source": [
    "# Step 2: Positional Encoding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "QMNlpmQ2aoCU"
   },
   "source": [
    "You are tasked with implementing a Positional Encoding layer in PyTorch, which will be used to inject information about the position of tokens in a sequence. This encoding is crucial for models like the Transformer, which lacks any inherent notion of token position in its architecture.\n",
    "1.Positional Encoding Initialization:\n",
    "\n",
    "The PositionalEncoding class will be initialized with the following parameters:\n",
    "d_model: The dimension of the embeddings (e.g., size of word vectors).\n",
    "max_len: The maximum sequence length that can be processed (default is 5000).\n",
    "The positional encoding should be created based on the formula using sine and cosine functions.\n",
    "\n",
    "2.Encoding Calculation:\n",
    "\n",
    "You need to calculate the positional encoding matrix using the suitable formula for even indices and odd indices.\n",
    "\n",
    "This positional encoding matrix will be added to the input sequence in the forward method.\n",
    "\n",
    "3.Forward Pass:\n",
    "\n",
    "In the forward method, the input tensor x will be added to the positional encoding corresponding to its sequence length.\n",
    "The output tensor will be the sum of the input tensor and the positional encoding, which gives the input tokens with position information.\n",
    "Expected Input and Output:\n",
    "Given an input sequence of shape (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "z2-SNJeLaoCU",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d0ff14153214f8c0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVZH8oBEaoCV"
   },
   "source": [
    "# Test Case 1 : Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQQLg_gEaoCV",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-70f4cecc8b08269b",
     "locked": true,
     "points": 100,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "0fb8b26d-f69d-4e24-cb27-13b872fbbb8f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1 :All positional encoding tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding Test Case\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def test_positional_encoding():\n",
    "    d_model = 16  # Smaller model for testing\n",
    "    max_len = 10  # Small sequence length\n",
    "    pos_enc = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "    # Create a mock input tensor\n",
    "    x = torch.zeros((1, max_len, d_model))  # Batch size = 1, seq_len = 10, d_model = 16\n",
    "\n",
    "    # Forward pass\n",
    "    output = pos_enc(x)\n",
    "\n",
    "    # Check if the output shape is correct\n",
    "    assert output.shape == x.shape, \"Output shape mismatch\"\n",
    "\n",
    "    # Check if the encoding changes the input\n",
    "    assert not torch.allclose(output, x), \"Positional encoding is not modifying the input\"\n",
    "\n",
    "    # Check for consistency in encoding values\n",
    "    encoding1 = pos_enc.encoding[:, :max_len].detach().numpy()\n",
    "    encoding2 = pos_enc.encoding[:, :max_len].detach().numpy()\n",
    "    assert np.allclose(encoding1, encoding2), \"Positional encoding is not consistent across calls\"\n",
    "\n",
    "    print(\"Test Case 1 :All positional encoding tests passed!\")\n",
    "\n",
    "# Run test case\n",
    "test_positional_encoding()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9PX4WaSaoCW"
   },
   "source": [
    "# Step 3: Multi-Head Attention"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "YbuFShd1aoCW"
   },
   "source": [
    "Your task is to implement the MultiHeadAttention class, a key component of the Transformer model. This module enables the model to focus on different parts of the input sequence simultaneously using multiple attention heads.\n",
    "\n",
    "You will implement the following functionalities:\n",
    "1.Initialization:\n",
    "\n",
    "    Define the parameters d_model (input and output dimensionality) and num_heads (number of attention heads).\n",
    "    Compute d_k and d_v, the dimensionality of each attention head.\n",
    "    Define linear transformations for computing query (self.query), key (self.key), and value (self.value).\n",
    "    Define the final fully connected layer (self.fc) to project the concatenated outputs back to d_model.\n",
    "2.Forward Pass:\n",
    "\n",
    "    Compute query (q), key (k), and value (v) vectors by applying the respective linear transformations.\n",
    "    Reshape q, k, and v to split them across multiple heads, ensuring each head gets d_k and d_v dimensions.\n",
    "\n",
    "    Compute scaled dot-product attention using the formula:\n",
    "\n",
    "     Apply a mask if provided (mask), ensuring masked positions do not influence attention.\n",
    "\n",
    "    Compute attention weights (attn_weights) using the Softmax function.\n",
    "\n",
    "    Apply the attention weights to the value (v) tensor, producing attention_output.\n",
    "\n",
    "    Reshape and concatenate all attention heads to get back to the original d_model dimension.\n",
    "\n",
    "    Pass the result through the final fully connected layer (self.fc) to obtain the final attention output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4FpV6xJ0aoCX",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-29bab5f4ee918a17",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.d_v = d_model // num_heads\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        q = self.query(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.key(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.value(x).view(batch_size, seq_len, self.num_heads, self.d_v).transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float))\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))  # Ensure mask matches input length\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        attention_output = torch.matmul(attn_weights, v).transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.fc(attention_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuR9Rn4UaoCX"
   },
   "source": [
    "# Test Case 2 Check multi_head_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZA3wo5baoCX",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-190e4c0b25291225",
     "locked": true,
     "points": 100,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "e83e6f66-9a9f-4172-9c9e-55c50ba040d7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 2:All Multi-Head Attention tests passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def test_multi_head_attention():\n",
    "    d_model = 16  # Small model for testing\n",
    "    num_heads = 4  # Number of heads\n",
    "    seq_len = 5  # Sequence length\n",
    "    batch_size = 2  # Batch size\n",
    "\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    # Create a random input tensor\n",
    "    x = torch.rand((batch_size, seq_len, d_model))\n",
    "\n",
    "    # Create a random mask\n",
    "    mask = torch.randint(0, 2, (batch_size, num_heads, seq_len, seq_len))\n",
    "\n",
    "    # Forward pass\n",
    "    output = mha(x, mask)\n",
    "\n",
    "    # Test 1: Check multi-head splits\n",
    "    q = mha.query(x).view(batch_size, seq_len, num_heads, d_model // num_heads).transpose(1, 2)\n",
    "    assert q.shape == (batch_size, num_heads, seq_len, d_model // num_heads), \"Query split is incorrect\"\n",
    "\n",
    "    # Test 2: Check softmax attention weights sum to 1\n",
    "    attn_scores = torch.matmul(q, q.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_model // num_heads, dtype=torch.float))\n",
    "    attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "    attn_sums = attn_weights.sum(dim=-1)\n",
    "    assert torch.allclose(attn_sums, torch.ones_like(attn_sums), atol=1e-5), \"Attention weights do not sum to 1\"\n",
    "\n",
    "    print(\"Test Case 2:All Multi-Head Attention tests passed!\")\n",
    "\n",
    "# Run test case\n",
    "test_multi_head_attention()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLxiB0LPaoCY"
   },
   "source": [
    "# Step 4: Feed-Forward Network"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "5UhQ9BqVaoCY"
   },
   "source": [
    "You are tasked with implementing a Feed-Forward Network (FFN), a critical part of the Transformer architecture. The FFN processes each token's representation independently, consisting of two fully connected layers. The first layer transforms the input features into a larger space (d_ff), and the second layer projects it back to the original feature dimension (d_model). A ReLU activation function is applied between these two layers.\n",
    "\n",
    "1.Initialization:\n",
    "    The FeedForward class should be initialized with:\n",
    "    d_model: The dimensionality of the input and output feature vectors.\n",
    "    d_ff: The dimensionality of the hidden layer (default value is 512).\n",
    "\n",
    "2.Forward Pass:\n",
    "    The forward pass will apply two fully connected layers:\n",
    "    First, the input tensor x (of shape [batch_size, seq_len, d_model]) will be passed through a linear layer (fc1) that projects it to the higher dimension d_ff.\n",
    "    Then, a ReLU activation function is applied to the output of fc1.\n",
    "    Finally, the result will be passed through a second linear layer (fc2), which projects the output back to the original dimension d_model.\n",
    "3.Expected Input and Output:\n",
    "    Given an input tensor x of shape [batch_size, seq_len, d_model], the output will be the same shape [batch_size, seq_len, d_model], but with transformed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EjB5U4aWaoCY",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a2ecdf8a3b200e2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=512):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62c6Z6Q4aoCY"
   },
   "source": [
    "# Step 5: Encoder Layer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "8fRzTC3gaoCY"
   },
   "source": [
    "You are tasked with implementing an Encoder Layer for a Transformer model. The encoder layer consists of two main components:\n",
    "\n",
    "Multi-Head Attention: This mechanism allows the model to focus on different parts of the input sequence simultaneously.\n",
    "Feed-Forward Network (FFN): This component applies two fully connected layers to each position independently.\n",
    "In addition, each of these components is followed by Layer Normalization and Residual Connections to stabilize training.\n",
    "\n",
    "Task Breakdown:\n",
    "1.Initialization:\n",
    "    The EncoderLayer class should be initialized with:\n",
    "    d_model: The dimensionality of the input and output features.\n",
    "    num_heads: The number of attention heads in the multi-head attention mechanism.\n",
    "    d_ff: The dimensionality of the hidden layer in the feed-forward network (default is 512).\n",
    "    The following components should be initialized:\n",
    "    multihead_attn: A MultiHeadAttention layer.\n",
    "    feedforward: A FeedForward layer.norm1, norm2: Two Layer Normalization layers.\n",
    "\n",
    "2.Forward Pass:\n",
    "The input tensor x is first passed through the multi-head attention mechanism (multihead_attn). The output is added to the original input x (residual connection), and then Layer Normalization (norm1) is applied.\n",
    "The resulting tensor is then passed through the feed-forward network (feedforward), and again the residual connection is added followed by Layer Normalization (norm2).\n",
    "The output of this forward pass will be the transformed tensor with attention and feed-forward operations applied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uOC_1sYWaoCY",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ac021edf972a02a8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 5: Encoder Layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff=512):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.multihead_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feedforward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.multihead_attn(x, mask)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8rGcDrcaoCZ"
   },
   "source": [
    "# Test Case 3: Check Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-F58_OzaoCZ",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-975f0ea7583cd5ff",
     "locked": true,
     "points": 100,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a65cf0ad-e45a-4ede-ce22-921b3486ac48",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 3:EncoderLayer tests passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def test_encoder_layer():\n",
    "    d_model = 16  # Model dimension\n",
    "    num_heads = 4  # Number of attention heads\n",
    "    seq_len = 5  # Sequence length\n",
    "    batch_size = 2  # Batch size\n",
    "\n",
    "    encoder_layer = EncoderLayer(d_model, num_heads)\n",
    "\n",
    "    # Create a random input tensor\n",
    "    x = torch.rand((batch_size, seq_len, d_model))\n",
    "\n",
    "    # Create a random mask\n",
    "    mask = torch.randint(0, 2, (batch_size, num_heads, seq_len, seq_len))\n",
    "\n",
    "    # Forward pass\n",
    "    output = encoder_layer(x, mask)\n",
    "\n",
    "    # Test 1: Check that MultiHeadAttention modifies input\n",
    "    attn_output = encoder_layer.multihead_attn(x, mask)\n",
    "    assert not torch.equal(attn_output, x), \"MultiHeadAttention does not modify input\"\n",
    "\n",
    "    # Test 2: Check that MultiHeadAttention modifies input\n",
    "    x = torch.rand(10, 20, d_model)\n",
    "    output = encoder_layer(x)\n",
    "    assert torch.all(output.std(dim=-1) > 0), \"LayerNorm not applied correctly!\"\n",
    "\n",
    "    print(\"Test Case 3:EncoderLayer tests passed!\")\n",
    "\n",
    "# Run test case\n",
    "test_encoder_layer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14i4SZi3aoCZ"
   },
   "source": [
    "# Step 6: Decoder Layer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "pRdqm-QwaoCZ"
   },
   "source": [
    "You need to implement the DecoderLayer class with the following requirements:\n",
    "\n",
    "1.Initialization (__init__ method):\n",
    "    Accepts d_model (hidden dimension), num_heads (number of attention heads), and d_ff (hidden size for the feed-forward network, default is 512).\n",
    "Initializes:\n",
    "    multihead_attn1: First multi-head self-attention layer (masked self-attention).\n",
    "    multihead_attn2: Second multi-head attention layer (cross-attention with encoder output).\n",
    "    feedforward: Fully connected feed-forward network.\n",
    "    norm1, norm2, and norm3: Three Layer Normalization layers.\n",
    "\n",
    "Forward Pass (forward method):\n",
    "Accepts the following inputs:\n",
    "    x: Target sequence input tensor (shape: [batch_size, tgt_seq_len, d_model]).\n",
    "    encoder_output: Output from the encoder (shape: [batch_size, src_seq_len, d_model]).\n",
    "    tgt_mask: Mask to prevent looking at future tokens in the target sequence.\n",
    "    src_mask: Mask to ignore padding in the encoder’s output.\n",
    "    Applies:\n",
    "    1.Masked Multi-Head Self-Attention (multihead_attn1):\n",
    "    Ensures the model attends only to previous words in the target sequence.\n",
    "    Output(attn_output1) is added to x (residual connection) and normalized with norm1.\n",
    "    2.Multi-Head Attention (multihead_attn2):\n",
    "    Attends to the encoder’s output to retrieve relevant information.\n",
    "    Output(attn_output2) is added to x (residual connection) and normalized with norm2.\n",
    "    3.Feed-Forward Network (feedforward):\n",
    "    Processes the result through fully connected layers.\n",
    "    Output(ff_output) is added to x (residual connection) and normalized with norm3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "p6Qh8qHtaoCa",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a089f6447080b612",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 6: Decoder Layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff=512):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.multihead_attn1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.multihead_attn2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feedforward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_output, tgt_mask=None, src_mask=None):\n",
    "        attn_output1 = self.multihead_attn1(x, tgt_mask)\n",
    "        x = self.norm1(x + attn_output1)\n",
    "        attn_output2 = self.multihead_attn2(x, src_mask)\n",
    "        x = self.norm2(x + attn_output2)\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = self.norm3(x + ff_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8pJ098MaoCa"
   },
   "source": [
    "# Test Case 4 : Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kLUlCH1GaoCa",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a3f9189dca98c048",
     "locked": true,
     "points": 100,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "0f6effcc-7597-476b-b579-0addb42ee30e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 4 Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_decoder_layer():\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    seq_length = 10\n",
    "    batch_size = 4\n",
    "\n",
    "    # Create a random tensor as input\n",
    "    x = torch.rand(batch_size, seq_length, d_model)\n",
    "    encoder_output = torch.rand(batch_size, seq_length, d_model)\n",
    "\n",
    "    # Initialize the DecoderLayer\n",
    "    decoder_layer = DecoderLayer(d_model, num_heads)\n",
    "    assert hasattr(decoder_layer, \"multihead_attn2\"), \"DecoderLayer must have a cross-attention layer\"\n",
    "    assert hasattr(decoder_layer, \"feedforward\"), \"DecoderLayer must have a feed-forward network\"\n",
    "    # Forward pass\n",
    "    output = decoder_layer(x, encoder_output)\n",
    "\n",
    "    # Assertions\n",
    "    assert output.shape == x.shape, \"Output shape mismatch\"\n",
    "    assert not torch.isnan(output).any(), \"Output contains NaN values\"\n",
    "\n",
    "# Run the test\n",
    "test_decoder_layer()\n",
    "print(\"Test Case 4 Passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Urr7FTUuaoCa"
   },
   "source": [
    "# Step 7: Transformer Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "qx2imrjXaoCb"
   },
   "source": [
    "Your task is to implement a Transformer model from scratch in PyTorch, which can process an input sequence (src) and generate an output sequence (tgt). The Transformer consists of an encoder-decoder architecture where:\n",
    "\n",
    "The Encoder processes the input (src) and generates a contextualized representation.\n",
    "The Decoder takes this representation along with the target (tgt) to generate the output.\n",
    "You need to implement the Transformer class using the provided EncoderLayer and DecoderLayer modules.\n",
    "\n",
    "\n",
    "1.Initialize the following layers in the __init__ method:\n",
    "\n",
    "    An embedding layer (self.embedding) to convert input token indices into vector representations.\n",
    "    A positional encoding layer (self.pos_encoder) to provide positional information.\n",
    "    A stack of encoder layers (self.encoder_layers) using nn.ModuleList.\n",
    "    A stack of decoder layers (self.decoder_layers) using nn.ModuleList.\n",
    "    A fully connected output layer (self.fc_out) that maps the decoder output to the vocabulary space.\n",
    "\n",
    "2.Implement the forward method:\n",
    "    Embed the src and tgt sequences and apply positional encoding.\n",
    "    Pass src through the encoder layers sequentially.\n",
    "    Pass tgt through the decoder layers sequentially, using the encoder output.\n",
    "    Apply the final linear layer to get the output logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Wppuo5a5aoCb",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9dde5c6dd1daa905",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 7: Transformer Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_encoder_layers, num_decoder_layers, max_len=5000):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads) for _ in range(num_encoder_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads) for _ in range(num_decoder_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, tgt_mask=None):\n",
    "        src = self.pos_encoder(self.embedding(src))\n",
    "        tgt = self.pos_encoder(self.embedding(tgt))\n",
    "\n",
    "        encoder_output = src\n",
    "        for layer in self.encoder_layers:\n",
    "            encoder_output = layer(encoder_output)\n",
    "\n",
    "        decoder_output = tgt\n",
    "        for layer in self.decoder_layers:\n",
    "            decoder_output = layer(decoder_output, encoder_output, tgt_mask=tgt_mask)\n",
    "\n",
    "        output = self.fc_out(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8oEPyBVaoCb"
   },
   "source": [
    "# Step 8: Prediction Module"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "Lr_PP1zUaoCg"
   },
   "source": [
    "Your task is to implement the translate function, which takes an English sentence as input and returns the translated French sentence using a Transformer model.\n",
    "\n",
    "This function should perform the following steps:\n",
    "1.Convert the input sentence into tokens using word_map_en.\n",
    "2.Create a target tensor initialized with zeros.\n",
    "3.Use the Transformer model to generate a translated sentence.\n",
    "4.Apply Softmax to compute token probabilities.\n",
    "5.Use Argmax to determine the most probable token at each position.\n",
    "6.Convert token indices back to words using word_map_fr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "h1yohb8WaoCg",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b78400dfce5aba3f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 8: Prediction Module\n",
    "def translate(input_sentence, word_map_en, word_map_fr, transformer):\n",
    "    # Tokenize input sentence\n",
    "    input_tensor = tokenize(input_sentence, word_map_en).unsqueeze(0)  # Shape (1, seq_len)\n",
    "\n",
    "    # Generate a mask for the target sentence\n",
    "    tgt_mask = torch.tril(torch.ones((input_tensor.size(1), input_tensor.size(1)))).unsqueeze(0).unsqueeze(0)  # Lower triangular mask\n",
    "\n",
    "    # Initialize a tensor for the target sentence\n",
    "    target_tensor = torch.zeros((1, input_tensor.size(1)), dtype=torch.long)\n",
    "\n",
    "    # Predict the output sentence (translation)\n",
    "    output = transformer(input_tensor, target_tensor, tgt_mask)\n",
    "\n",
    "    # Apply Softmax to get probabilities\n",
    "    softmax_output = F.softmax(output, dim=-1)\n",
    "\n",
    "    # Get predicted token indices (Argmax)\n",
    "    predicted_tokens = torch.argmax(softmax_output, dim=-1)\n",
    "\n",
    "    # Convert predicted tokens back to words\n",
    "    reverse_word_map_fr = {v: k for k, v in word_map_fr.items()}\n",
    "    translated_sentence = [reverse_word_map_fr[token.item()] for token in predicted_tokens[0] if token != 0]\n",
    "\n",
    "    return \" \".join(translated_sentence)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "5O9nQXpNaoCh"
   },
   "source": [
    "# Step 9: Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "tdiK1X_GaoCh",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7396e824a4beef54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 9: Initialize Model\n",
    "vocab_size_en = len(word_map_en)\n",
    "vocab_size_fr = len(word_map_fr)\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "\n",
    "transformer = Transformer(vocab_size_en, d_model, num_heads, num_encoder_layers, num_decoder_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuJVZT1UaoCh"
   },
   "source": [
    "# Step 10: Take Input Sentence and Generate Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9kIDvGxaoCh",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0bb5990170158468",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "ffa3ae05-2577-4fff-c546-a5de73dc9c46",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: I love AI .\n",
      "Predicted Translation: ధోని ధోని ధోని ధోని\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Take Input Sentence and Generate Translation\n",
    "input_sentence = \"I love AI .\"  # Example input sentence\n",
    "predicted_sentence = translate(input_sentence, word_map_en, word_map_fr, transformer)\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Predicted Translation:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCfPaov0aoCh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmqzwnwzaoCi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
