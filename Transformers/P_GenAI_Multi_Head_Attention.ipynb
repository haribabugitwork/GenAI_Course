{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588dbdcc-264b-4c62-911f-55b706e08df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79320154-1f2e-42de-a26f-bcd3d1dcdcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(24)  # Python random seed\n",
    "np.random.seed(24)  # NumPy seed\n",
    "torch.manual_seed(24)  # PyTorch seed (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2745f2-4ca8-440f-8ea1-19f7637111f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set print options: No scientific notation, 2 decimal places\n",
    "torch.set_printoptions(sci_mode=False, precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e0cf65-434b-4fdc-95e4-458b3007c09d",
   "metadata": {},
   "source": [
    "# Define the maximum sequence length and the embedding dimension for a model:\n",
    "\n",
    "max_sequence_length = 5: Specifies the maximum number of tokens a sequence can have. If a sequence is shorter, it may be padded; if longer, it may be truncated.\n",
    "\n",
    "d_model = 8: Defines the size of each token’s embedding vector, meaning each token will be represented as a     8-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39030707-f514-4326-9f10-c8a9f2454aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 8\n",
    "max_sequence_length = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f85bce-72dd-43c4-9efa-9e122ab569c6",
   "metadata": {},
   "source": [
    "# Define three linear layers using nn.Linear in PyTorch:\n",
    "\n",
    "w_query: Projects input embeddings into query space.\n",
    "\n",
    "w_key: Projects input embeddings into key space.\n",
    "\n",
    "w_value: Projects input embeddings into value space.\n",
    "## These linear layers transform input embeddings (d_model dimensional) into new representations of the same size (d_model → d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ce832d-90e0-4fec-9ab5-b644a3673976",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_query = nn.Linear(d_model, d_model)\n",
    "w_key   = nn.Linear(d_model, d_model)\n",
    "w_value = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6556b4-f01c-4a2f-9409-f97cfdc2884d",
   "metadata": {},
   "source": [
    "# Create a tensor tokens with random values, to simulate a batch of sequence of token embeddings.\n",
    "\n",
    "Use torch.randn() to generate a random tensor of shape (batch_size, max_sequence_length, d_model), where:\n",
    "\n",
    "batch_size defines the number of sequences processed simultaneously\n",
    "\n",
    "max_sequence_length represents the number of tokens in the sequence.\n",
    "\n",
    "d_model represents the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d78ba-b6a4-4245-8caf-ffcabcd32a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb185a63-2804-4c76-ac90-97f8da038197",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input_tokens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ef28ef-5897-4618-be30-06a43449774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c611a1bf-1ce6-4d2e-af5c-df825bc27bb4",
   "metadata": {},
   "source": [
    "# Apply linear transformations to the tokens tensor using w_query, w_key, and w_value to obtain query (q), key (k), and value (v) representations.\n",
    "\n",
    "## Pass tokens through the three linear layers to compute q, k, and v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c86ac56-c293-4f10-a5eb-b2e7196c4f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "531b88e5-baa8-427f-b486-fa144808cb66",
   "metadata": {},
   "source": [
    "# Compute Per-Head Dimensions in Multi-Head Attention\n",
    "In multi-head attention, we split the embedding dimension (d_model) into multiple heads to allow the model to focus on different parts of the input simultaneously.\n",
    "\n",
    "Define the number of attention heads as num_heads = 4.\n",
    "\n",
    "Compute the per-head query (d_q), key (d_k), and value (d_v) dimensions by dividing d_model by num_heads.\n",
    "\n",
    "Ensure that d_q, d_k, and d_v are equal and represent the dimension per head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28805687-d9f7-4709-91a4-20c6eff2feb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a559fc97-533e-4105-b5b6-0f51cd0c5057",
   "metadata": {},
   "source": [
    "# Reshape each tensor to (batch_size, max_sequence_length, num_heads, d_q), where:\n",
    "\n",
    "batch_size is the number of input sequences in a batch.\n",
    "\n",
    "max_sequence_length is the number of tokens per sequence.\n",
    "\n",
    "num_heads is the number of attention heads.\n",
    "\n",
    "d_q, d_k, and d_v are the per-head dimensions (d_model // num_heads).\n",
    "\n",
    "Verify that the new shape correctly divides the d_model dimension across multiple heads.\n",
    "\n",
    "Print the new shapes of q, k, and v to confirm the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c639e-2faf-4c49-967b-dfdf779b042e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98e4b9f-9f69-443a-a761-85287daaafcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a29ee3-4a40-40eb-860d-ea9ecfe784d0",
   "metadata": {},
   "source": [
    "# Transpose Query Tensor for Multi-Head Attention\n",
    "In multi-head attention, after reshaping the query (q), key (k), and value (v) tensors, we need to transpose them to bring the num_heads dimension to the second position. This helps in efficiently computing attention scores across multiple heads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b6a79-5420-448a-a40f-317394deb9f1",
   "metadata": {},
   "source": [
    "Assume you have a query tensor (q) of shape (batch_size, max_sequence_length, num_heads, d_q) after reshaping.\n",
    "\n",
    "Transpose q to rearrange dimensions, so that num_heads moves to the second position, resulting in (batch_size, num_heads, max_sequence_length, d_q).\n",
    "\n",
    "Print the shape of q after transposing to verify the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f7636-0548-47cb-93ea-21b8a258e82d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb618295-c8df-4d89-82d3-efdd6bd84b7d",
   "metadata": {},
   "source": [
    "# Repeat the same operation for k and v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16876d4c-f466-44b8-b12d-b71fc05540f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5994cfa0-e830-47b1-bec8-1616f38cc340",
   "metadata": {},
   "source": [
    "For a single head:\n",
    "$$\n",
    "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{new V} = \\text{self attention}.V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d41c0-624e-49f6-b419-a7c211748485",
   "metadata": {},
   "source": [
    "# Compute Attention Scores for Each Head\n",
    "In multi-head attention, after transposing q and k, we compute attention scores using scaled dot-product attention. This involves:\n",
    "\n",
    "Taking the dot product of q with the transposed k to get similarity scores.\n",
    "\n",
    "Scaling the scores by dividing by the square root of d_k (to stabilize gradients).\n",
    "\n",
    "Printing the shape of attn_scores to verify it per head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540fd242-ee13-4ceb-9877-643f019bb0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff6ec9-dbb1-48ea-82f5-fc7b3af68282",
   "metadata": {},
   "outputs": [],
   "source": [
    "k.transpose(-1, -2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb853a84-f22f-41ea-af7b-d62118a3e205",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "- This is to ensure words don't get context from words generated in the future.\n",
    "- Not required in the encoders, but required in the decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e343aa37-d26b-4a82-9b4f-f80429972f13",
   "metadata": {},
   "source": [
    "# Create a lower triangular mask using torch.tril, which generates a matrix where only the lower triangle (including the diagonal) contains ones, while the upper triangle contains zeros. This mask is typically used in masked self-attention in transformers to ensure that each position in a sequence can only attend to previous positions and itself, preventing access to future tokens during decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c219ed-b852-4d9d-87ec-a6a9f0d62abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254ed455-7ebe-4570-b305-3d84dda869fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d533cbf-b289-4c3b-a8c3-65a2298c3118",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7220fc17-db20-47f7-841a-b088864811ea",
   "metadata": {},
   "source": [
    "# Compute the weighted sum of value (v) vectors using attention weights, where each query token receives a context-aware representation. This ensures that each generated token attends to relevant past tokens, influencing its prediction based on learned dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df49fc69-9964-4211-818e-11515ff41ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4109ddf-f398-45ce-a764-0d36bb3fd72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3959bb1-c655-4c3d-b43e-9bf114694d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c051c-b61f-4ec2-a48b-ceba9e023562",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed78eb-c8b4-4c02-923d-16772b0982a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f2dfc1-ca21-4a7d-bd5d-528cb7b77ed0",
   "metadata": {},
   "source": [
    "# Implement MultiHeadAttention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa5ab9-7d5e-4c50-ae3f-a67280be7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.d_v = d_model // num_heads\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q_input, k_input, v_input, mask=None):\n",
    "\n",
    "        batch_size, max_sequence_length, _ = q_input.size()\n",
    "        \n",
    "        Q = self.query(q_input)\n",
    "        K = self.key(k_input)\n",
    "        V = self.value(v_input)\n",
    "        \n",
    "        q = Q.reshape(batch_size, max_sequence_length, self.num_heads, self.d_k)\n",
    "        k = K.reshape(batch_size, max_sequence_length, self.num_heads, self.d_k)\n",
    "        v = V.reshape(batch_size, max_sequence_length, self.num_heads, self.d_v)\n",
    "        \n",
    "        q = q.transpose(1, 2) # [batch_size, num_heads, max_sequence_length, d_k]\n",
    "        k = k.transpose(1, 2) # [batch_size, num_heads, max_sequence_length, d_k]\n",
    "        v = v.transpose(1, 2) # [batch_size, num_heads, max_sequence_length, d_v]\n",
    "\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float))\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))  # Ensure mask matches input length\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "    \n",
    "        attention_output = torch.matmul(attn_weights, v).transpose(1, 2).contiguous().view(batch_size, max_sequence_length, self.d_model)\n",
    "       \n",
    "        output = self.fc(attention_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598cd88-6ecd-42f5-8f22-8f99bc5963f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 16  # Small model for testing\n",
    "num_heads = 4  # Number of heads\n",
    "seq_len = 5  # Sequence length\n",
    "batch_size = 2  # Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd2095-8588-47d4-ab65-443948cd6262",
   "metadata": {},
   "source": [
    "# Create a random input tensor with batch_size, seq_len, d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12a779-42d6-4c10-87c9-f6b530b5456e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6802e33-1c86-4b2e-af34-3c334661ad66",
   "metadata": {},
   "source": [
    "# Self-Attention in the Encoder\n",
    "## Purpose: Allows each token to attend to all other tokens in the input sequence.\n",
    "\n",
    "### Masking: No causal masking (tokens can see all positions).\n",
    "\n",
    "#### Input: x (same for query, key, value).\n",
    "\n",
    "#### Mask: Usually a padding mask (not needed for random input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c15f2-0127-44c2-b0c3-2af874b2bc82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b4eba8b-25b8-434a-ab86-286719b37d17",
   "metadata": {},
   "source": [
    "# Masked Self-Attention in the Decoder\n",
    "## Purpose: Prevents each token from attending to future tokens.\n",
    "\n",
    "### Masking: Causal mask applied.\n",
    "\n",
    "### Input: x (same for query, key, value).\n",
    "\n",
    "### Mask: Lower triangular mask to enforce causality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0237393b-c9a6-4b1f-8402-d6ad49232deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad8ea95a-597a-47d0-8778-b65cce26930e",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Cross-Attention in the Decoder\n",
    "## Purpose: Allows decoder tokens to attend to all encoder tokens.\n",
    "\n",
    "## Masking: No causal mask (attends to all encoder tokens).\n",
    "\n",
    "### Input:\n",
    "\n",
    "#### q (decoder representation).\n",
    "\n",
    "#### k, v (encoder output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911eb27a-cc6e-48c3-a091-733d903e78e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbfa0882-8562-4235-ab3a-70bc97c165fa",
   "metadata": {},
   "source": [
    "# Understanding Contiguous and Non-Contiguous Tensors in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614019a-8779-4ce7-83d2-c9852e0c6c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a contiguous tensor\n",
    "x = torch.randn(2, 3)\n",
    "print(\"Original tensor:\\n\", x)\n",
    "print(\"Is contiguous?\", x.is_contiguous())  # True\n",
    "\n",
    "# Transpose it (creates a non-contiguous tensor)\n",
    "x_t = x.transpose(0, 1)\n",
    "print(\"\\nTransposed tensor:\\n\", x_t)\n",
    "print(\"Is contiguous?\", x_t.is_contiguous())  # False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4214c9-c3f8-41be-aaa4-4eb60fdb075b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
