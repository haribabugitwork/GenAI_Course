<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Understanding QLoRA Quantization</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      margin: 20px auto;
      max-width: 900px;
      padding: 20px;
      background: #f9f9f9;
      color: #333;
    }
    h1, h2, h3 {
      color: #222;
    }
    code, pre {
      background: #eee;
      padding: 2px 4px;
      border-radius: 4px;
    }
    pre {
      padding: 10px;
      overflow-x: auto;
    }
    a {
      color: #0066cc;
    }
  </style>
</head>
<body>
  <h1>Understanding QLoRA Quantization</h1>
  <p><strong>Author:</strong> Hari Venkata</p>

  <h2>Introduction</h2>
  <p>Quantization is a key technique used in QLoRA to reduce the memory footprint and computational cost of large language models. It works by converting 32-bit floating-point (FP32) numbers into smaller bit-width formats like 8-bit integers (Int8) or 4-bit NormalFloat representations.</p>

  <h2>1. FP32 to Int8 Quantization</h2>
  <p>The simplest form of quantization involves converting a tensor from FP32 to Int8 by rescaling the values. The general steps are:</p>
  <ol>
    <li>Find the maximum absolute value in the tensor: <code>absmax(X^FP32)</code></li>
    <li>Define the quantization constant: <code>c^FP32 = 127 / absmax(X^FP32)</code></li>
    <li>Multiply and round the original tensor: <code>X^Int8 = round(X^FP32 * c^FP32)</code></li>
  </ol>

  <h3>Concrete Example</h3>
  <p>Let’s take a tensor:</p>
  <pre>[0.5, -1.0, 0.25, -0.75, 10]</pre>
  <p>Step 1: Find <code>absmax = 10</code><br>
  Step 2: Compute scale <code>c = 127 / 10 = 12.7</code><br>
  Step 3: Multiply and round:</p>
  <pre>round([0.5, -1.0, 0.25, -0.75, 10] * 12.7) => [6, -13, 3, -10, 127]</pre>

  <h2>2. Issues with Global Quantization</h2>
  <p>Using a single scale value for all model parameters can be very sensitive to outliers. A single large value can cause the scale to shrink too much, reducing the accuracy of smaller values after rounding.</p>

  <h2>3. Block-wise Quantization</h2>
  <p>To overcome outliers skewing the scale, we divide the parameters into blocks (e.g., of size 64) and compute a separate quantization constant for each block.</p>
  <p>This improves accuracy but increases memory usage since each block’s scale needs to be stored.</p>

  <h3>How much extra memory?</h3>
  <ul>
    <li>1 FP32 scale per 64 parameters = 0.5 bits per parameter</li>
    <li>If we quantize the scale values to Int8 = 0.127 bits per parameter</li>
  </ul>

  <h3>Example of Block Scale Quantization</h3>
  <pre>
Block 1: [0.5, 0.75, -1.0] => absmax = 1.0 => scale = 127.0
Block 2: [5, 7, -10] => absmax = 10 => scale = 12.7
  </pre>

  <h2>4. 4-bit NormalFloat Quantization</h2>
  <p>This is a more sophisticated format used in QLoRA that preserves more dynamic range in fewer bits. Instead of uniform rounding, it uses:</p>
  <ul>
    <li>1 sign bit</li>
    <li>3 bits for a shared exponent (per block)</li>
    <li>Several 4-bit mantissas for values</li>
  </ul>

  <p>This is efficient both for compression and for inference acceleration on certain hardware.</p>

  <h2>5. Summary</h2>
  <ul>
    <li>Quantization is essential for reducing memory and compute costs.</li>
    <li>Naive FP32 to Int8 quantization is a good start but sensitive to outliers.</li>
    <li>Block-wise quantization balances accuracy and memory.</li>
    <li>Scale quantization further reduces overhead.</li>
    <li>4-bit NormalFloat offers a powerful trade-off using log-scale quantization and shared exponents.</li>
  </ul>

  <p>As QLoRA shows, you don’t need to sacrifice accuracy to get massive efficiency gains—smart quantization strategies let us have both.</p>

  <p><a href="index.html">&larr; Back to Home</a></p>
</body>
</html>
